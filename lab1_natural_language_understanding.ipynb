{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a4a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics(chars, words, sents):\n",
    "    word_lens = [len(word) for word in words]\n",
    "    sent_lens = [len(sent) for sent in sents]\n",
    "    chars_in_sents = [len(''.join(sent)) for sent in sents]\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = max(sent_lens)\n",
    "    longest_word = max(word_lens)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1fdf69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: [\n",
      "words: [\n",
      "sents: ['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']\n",
      "Word per sentence 20\n",
      "Char per word 3\n",
      "Char per sentence 68\n",
      "Longest sentence 204\n",
      "Longest word 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "alice_chars = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "print('chars:', alice_chars[0])\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "print('words:', alice_words[0])\n",
    "alice_sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "print('sents:', alice_sents[0])\n",
    "def statistics(chars, words, sents):\n",
    "    word_lens = [len(word) for word in words]\n",
    "    sent_lens = [len(sent) for sent in sents]\n",
    "    chars_in_sents = [len(''.join(sent)) for sent in sents]\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = max(sent_lens)\n",
    "    longest_word = max(word_lens)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word = statistics(alice_chars, alice_words, alice_sents)\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word)\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sentence)\n",
    "print('Longest word', longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e85ae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2636"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set([w.lower() for w in alice_words])\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf3adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "alice_freq_list = Counter(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6267b72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_list.get('ALL', 0))\n",
    "print(alice_freq_list.get('All', 0))\n",
    "print(alice_freq_list.get('all', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326b98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_freq_dist = nltk.FreqDist(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1373b502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_dist.get('ALL', 0))\n",
    "print(alice_freq_dist.get('All', 0))\n",
    "print(alice_freq_dist.get('all', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc22f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 1993, \"'\": 1731, 'the': 1642, 'and': 872, '.': 764}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def nbest(d, n=1):\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "alice_lowercase_freq_list = Counter([w.lower() for w in alice_words])\n",
    "best_words = nbest(alice_lowercase_freq_list,n=5)\n",
    "print(best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc46e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "CutOFF Min: 2.0 MAX: 100.0  Lexicon Size: 1453\n",
      "Original 2636\n",
      "CutOFF Min: 2.0 MAX: inf  Lexicon Size: 1503\n",
      "Original 2636\n",
      "CutOFF Min: -inf MAX: 100.0  Lexicon Size: 2586\n",
      "Original 2636\n",
      "CutOFF Min: -inf MAX: inf  Lexicon Size: 2636\n"
     ]
    }
   ],
   "source": [
    "def cut_off(vocab, n_min=100, n_max=100):\n",
    "    new_vocab = []\n",
    "    for word, count in vocab.items():\n",
    "        if count >= n_min and count <= n_max:\n",
    "            new_vocab.append(word)\n",
    "    return new_vocab\n",
    "\n",
    "lower_bound = float(2.0) # Change these two number to compute the required cut offs\n",
    "upper_bound = float(100.0)\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)\n",
    "\n",
    "lower_bound = float(2) # Change these two number to compute the required cut offs\n",
    "upper_bound = float(\"inf\")\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)\n",
    "\n",
    "lower_bound = float(\"-inf\") # Change these two number to compute the required cut offs\n",
    "upper_bound = float(100)\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)\n",
    "\n",
    "lower_bound = float(\"-inf\") # Change these two number to compute the required cut offs\n",
    "upper_bound = float(\"inf\")\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b956303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 326\n",
      "NLTK: 179\n",
      "sklearn: 318\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
    "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
    "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6491765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "No stopwords 2490\n",
      "Top100 overlap 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "alice_vocab = set([w.lower() for w in alice_words])\n",
    "top100 = list(nbest(alice_lowercase_freq_list,n=100).keys())\n",
    "stop_words = NLTK_STOP_WORDS\n",
    "\n",
    "overlap = set(top100).intersection(stop_words)#Compute the intersation between top100 and stop_words\n",
    "alice_vocab_no_stopwords = set(alice_vocab).difference(stop_words)# Remove Stopwords from alice vocab\n",
    "\n",
    "\n",
    "print('Original', len(alice_vocab))\n",
    "print('No stopwords', len(alice_vocab_no_stopwords))\n",
    "print('Top100 overlap', len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea31c21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAnother Corpus\u001b[0m\n",
      "chars: [\n",
      "words: [\n",
      "sents: ['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', '1667', ']']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mAnother Corpus\\033[0m\")\n",
    "milton_chars = nltk.corpus.gutenberg.raw('milton-paradise.txt')\n",
    "print('chars:', milton_chars[0])\n",
    "milton_words = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
    "print('words:', milton_words[0])\n",
    "milton_sents = nltk.corpus.gutenberg.sents('milton-paradise.txt')\n",
    "print('sents:', milton_sents[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8a9625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDescriptive statistics on the reference sentences and tokens\u001b[0m\n",
      "Word per sentence 52\n",
      "Char per word 4\n",
      "Char per sentence 203\n",
      "Longest sentence 533\n",
      "Longest word 16\n"
     ]
    }
   ],
   "source": [
    "word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word = statistics(milton_chars, milton_words, milton_sents)\n",
    "\n",
    "print(\"\\033[1mDescriptive statistics on the reference sentences and tokens\\033[0m\")\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word)\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sentence)\n",
    "print('Longest word', longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f781308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: '['\n",
      "first sentence: '[Paradise Lost by John Milton 1667] \n",
      " \n",
      " \n",
      "'\n",
      "107373\n",
      "2234\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "# un-comment the lines above, if you get 'ModuleNotFoundError'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txt = milton_chars\n",
    "# process the document\n",
    "doc = nlp(txt)\n",
    "\n",
    "\n",
    "\n",
    "print(\"first token: '{}'\".format(doc[0]))\n",
    "print(\"first sentence: '{}'\".format(list(doc.sents)[0]))\n",
    "# access list of tokens (Token objects)\n",
    "print(len(doc))\n",
    "# access list of sentences (Span objects)\n",
    "print(len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4c354f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDescriptive statistics on the automatically processed corpus by spacy\u001b[0m\n",
      "Word per sentence 1\n",
      "Char per word 4\n",
      "Char per sentence 1\n",
      "Longest sentence 1\n",
      "Longest word 63\n"
     ]
    }
   ],
   "source": [
    "word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word = statistics(txt, doc, format(list(doc.sents)))\n",
    "\n",
    "print(\"\\033[1mDescriptive statistics on the automatically processed corpus by spacy\\033[0m\")\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word)\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sentence)\n",
    "print('Longest word', longest_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bf1bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "milton_words_nltk = nltk.word_tokenize(milton_chars)\n",
    "milton_sents_nltk = nltk.sent_tokenize(milton_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef117ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDescriptive statistics on the automatically processed corpus by nltk\u001b[0m\n",
      "Word per sentence 253\n",
      "Char per word 4\n",
      "Char per sentence 253\n",
      "Longest sentence 2658\n",
      "Longest word 18\n"
     ]
    }
   ],
   "source": [
    "word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word = statistics(milton_chars, milton_words_nltk, milton_sents_nltk)\n",
    "\n",
    "print(\"\\033[1mDescriptive statistics on the automatically processed corpus by nltk\\033[0m\")\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word)\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sentence)\n",
    "print('Longest word', longest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace2826c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLowercased lexicons for reference\u001b[0m\n",
      "9021\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "milton_lexicon = set([w.lower() for w in milton_words])\n",
    "\n",
    "print(\"\\033[1mLowercased lexicons for reference\\033[0m\")\n",
    "\n",
    "print(len(milton_lexicon))\n",
    "print('ALL' in milton_lexicon)\n",
    "print('All' in milton_lexicon)\n",
    "print('all' in milton_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca100db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLowercased lexicons for spacy\u001b[0m\n",
      "54\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "milton_lexicon_spacy = set([w.lower() for w in format(doc)])\n",
    "\n",
    "print(\"\\033[1mLowercased lexicons for spacy\\033[0m\")\n",
    "\n",
    "print(len(milton_lexicon_spacy))\n",
    "print('ALL' in milton_lexicon)\n",
    "print('All' in milton_lexicon)\n",
    "print('all' in milton_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bf6431c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mLowercased lexicons for nltk\u001b[0m\n",
      "9280\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "milton_lexicon_nltk = set([w.lower() for w in milton_words_nltk])\n",
    "\n",
    "print(\"\\033[1mLowercased lexicons for nltk\\033[0m\")\n",
    "\n",
    "print(len(milton_lexicon_nltk))\n",
    "print('ALL' in milton_lexicon)\n",
    "print('All' in milton_lexicon)\n",
    "print('all' in milton_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba9b4688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFrequency Distribution for reference\u001b[0m\n",
      "0\n",
      "96\n",
      "604\n",
      "Original 10751\n",
      "CutOFF Min: 96.0 MAX: 604.0  Lexicon Size: 108\n",
      "{',': 10198, 'and': 2799, 'the': 2505, ';': 2317, 'to': 1758}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "milton_freq_list = Counter(milton_words)\n",
    "\n",
    "print(\"\\033[1mFrequency Distribution for reference\\033[0m\")\n",
    "\n",
    "print(milton_freq_list.get('ALL', 0))\n",
    "print(milton_freq_list.get('All', 0))\n",
    "print(milton_freq_list.get('all', 0))\n",
    "\n",
    "lower_bound = float(96) # Change these two number to compute the required cut offs\n",
    "upper_bound = float(604)\n",
    "lexicon_cut_off = len(cut_off(milton_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(milton_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)\n",
    "\n",
    "best_words = nbest(milton_freq_list,n=5)\n",
    "print(best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b707cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFrequency Distribution for spacy\u001b[0m\n",
      "0\n",
      "0\n",
      "0\n",
      "Original 80\n",
      "CutOFF Min: 96.0 MAX: 604.0  Lexicon Size: 18\n",
      "{' ': 81196, 'e': 44815, 't': 29572, 'o': 26121, 'a': 24655}\n"
     ]
    }
   ],
   "source": [
    "milton_freq_list = Counter(format(doc))\n",
    "\n",
    "print(\"\\033[1mFrequency Distribution for spacy\\033[0m\")\n",
    "\n",
    "print(milton_freq_list.get('ALL', 0))\n",
    "print(milton_freq_list.get('All', 0))\n",
    "print(milton_freq_list.get('all', 0))\n",
    "\n",
    "lower_bound = float(96) # Change these two number to compute the required cut offs\n",
    "upper_bound = float(604)\n",
    "lexicon_cut_off = len(cut_off(milton_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(milton_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)\n",
    "\n",
    "best_words = nbest(milton_freq_list,n=5)\n",
    "print(best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dad417c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFrequency Distribution for nltk\u001b[0m\n",
      "0\n",
      "96\n",
      "595\n",
      "Original 10986\n",
      "CutOFF Min: 96.0 MAX: 604.0  Lexicon Size: 105\n",
      "{',': 10228, 'and': 2799, 'the': 2505, ';': 2326, 'to': 1758}\n"
     ]
    }
   ],
   "source": [
    "milton_freq_list = Counter(milton_words_nltk)\n",
    "\n",
    "print(\"\\033[1mFrequency Distribution for nltk\\033[0m\")\n",
    "\n",
    "print(milton_freq_list.get('ALL', 0))\n",
    "print(milton_freq_list.get('All', 0))\n",
    "print(milton_freq_list.get('all', 0))\n",
    "\n",
    "lower_bound = float(96) # Change these two number to compute the required cut offs\n",
    "upper_bound = float(604)\n",
    "lexicon_cut_off = len(cut_off(milton_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(milton_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)\n",
    "\n",
    "best_words = nbest(milton_freq_list,n=5)\n",
    "print(best_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f202fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
