{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download(\"subjectivity\")\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "mr = movie_reviews\n",
    "\n",
    "\n",
    "from nltk.corpus import subjectivity\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer, VaderConstants\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "doc = subjectivity.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7827295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol2str(doc):\n",
    "    # flatten & join\n",
    "    return \" \".join([w for sent in doc for w in sent])\n",
    "\n",
    "def score_sent(document, use_pos=False):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    obj = []\n",
    "    for sent in document:\n",
    "        if use_pos:\n",
    "            tagged_sent = pos_tag(sent, tagset='universal')\n",
    "        else:\n",
    "            tagged_sent = [(w, None) for w in sent]\n",
    "\n",
    "        for tok, tag in tagged_sent:\n",
    "            ss = lesk(sent, tok, pos=pos2wn.get(tag, None))\n",
    "            if ss:\n",
    "                sense = swn.senti_synset(ss.name())\n",
    "                pos.append(sense.pos_score())\n",
    "                neg.append(sense.neg_score())\n",
    "                obj.append(sense.obj_score())\n",
    "    return pos, neg, obj\n",
    "\n",
    "def subjectivity_sentence_level(document, analyzer):\n",
    "    S = 0\n",
    "    O = 0\n",
    "    labels = ['S', 'O']\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] != 0:\n",
    "            S+=1\n",
    "        else:\n",
    "            O+=1\n",
    "    return labels[np.argmax(np.asarray([S, O]))]\n",
    "def rm_objective_sentences(document, analyzer):\n",
    "    new_doc = []\n",
    "    ob_doc = []\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] != 0:\n",
    "            new_doc.append(\" \".join(sentence))\n",
    "        else:\n",
    "            ob_doc.append(\" \".join(sentence))\n",
    "    return new_doc, ob_doc\n",
    "def polarity_doc_level(document, analyzer):\n",
    "    value = analyzer.polarity_scores(document)\n",
    "    P = 0\n",
    "    N = 0\n",
    "    labels = ['P', 'N']\n",
    "    if value[\"compound\"] > 0:\n",
    "        P += 1\n",
    "    elif value[\"compound\"] <= 0: # In this way we penalize the neg class\n",
    "        N += 1\n",
    "    return labels[np.argmax(np.asarray([P, N]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test with Stratified K Fold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores_clf = []\n",
    "scores_subjectivity = []\n",
    "\n",
    "\n",
    "#corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref_sub, ref_ob = rm_objective_sentences(doc, analyzer)\n",
    "\n",
    "ref = numpy.array([0] * len(ref_ob) + [1] * len(ref_sub))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(doc, ref)):\n",
    "    x_train, x_test = [doc[indx] for indx in train_index], [doc[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    scores = cross_validate(clf, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    hyp_sentence = [subjectivity_sentence_level(doc, analyzer) for doc in test_x_split]\n",
    "    scores_subjectivity.append(f1_score(y_test, hyp_sentence, average='macro'))\n",
    "        \n",
    "    \n",
    "print('F1 classifier:', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('F1 Subjectivity:',  round(sum(scores_subjectivity)/len(scores_subjectivity), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ae960",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf = []\n",
    "scores_subjectivity = []\n",
    "\n",
    "ref_sub, ref_ob = rm_objective_sentences(mr, analyzer)\n",
    "\n",
    "ref = numpy.array([0] * len(ref_ob) + [1] * len(ref_sub))\n",
    "new_mr = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(mr, ref)):\n",
    "    x_train, x_test = [mr[indx] for indx in train_index], [mr[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    scores = cross_validate(clf, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    new_mr.append(hyp)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    hyp_sentence = [subjectivity_sentence_level(mr, analyzer) for mr in test_x_split]\n",
    "    scores_subjectivity.append(f1_score(y_test, hyp_sentence, average='macro'))\n",
    "        \n",
    "    \n",
    "print('F1 classifier:', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('F1 Subjectivity:',  round(sum(scores_subjectivity)/len(scores_subjectivity), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c56ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test with Stratified K Fold\n",
    "\n",
    "scores_clf = []\n",
    "scores_polarity = []\n",
    "\n",
    "\n",
    "rev_wo_ob_neg = new_mr.paras(categories='neg')\n",
    "rev_wo_ob_pos = new_mr.paras(categories='pos')\n",
    "#corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref = numpy.array([0] * len(rev_wo_ob_neg) + [1] * len(rev_wo_ob_pos))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(new_mr, ref)):\n",
    "    x_train, x_test = [new_mr[indx] for indx in train_index], [new_mr[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    scores = cross_validate(clf, vectors, ref, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    hyp_polarity = [polarity_doc_level(new_mr, analyzer) for new_mr in x_test]\n",
    "    scores_polarity.append(f1_score(y_test, hyp_polarity, average='macro'))\n",
    "\n",
    "    \n",
    "    \n",
    "print('F1 classifier Polarity w/o Obj:', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('F1 Polarity:',  round(sum(scores_polarity)/len(scores_polarity), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4df7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf = []\n",
    "scores_polarity = []\n",
    "\n",
    "rev_neg = mr.paras(categories='neg')\n",
    "rev_pos = mr.paras(categories='pos')\n",
    "#corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref = numpy.array([0] * len(rev_neg) + [1] * len(rev_pos))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(mr, ref)):\n",
    "    x_train, x_test = [mr[indx] for indx in train_index], [mr[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    scores = cross_validate(clf, vectors, ref, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    hyp_polarity = [polarity_doc_level(mr, analyzer) for mr in x_test]\n",
    "    scores_polarity.append(f1_score(y_test, hyp_polarity, average='macro'))\n",
    "\n",
    "    \n",
    "    \n",
    "print('F1 classifier Polarity with Obj:', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('F1 Polarity:',  round(sum(scores_polarity)/len(scores_polarity), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(doc):\n",
    "    aspects = []\n",
    "    asp_doc = []\n",
    "    for sent in doc.sents:\n",
    "        target = None\n",
    "        opinion = None\n",
    "        for tok in sent:\n",
    "            if tok.dep_ == 'nsubj' and tok.pos_ == 'NOUN':\n",
    "                target = tok.text\n",
    "            if tok.pos_ == 'ADJ':\n",
    "                descr = ''\n",
    "                for child in tok.children:\n",
    "                    if child.pos_ != 'ADV':\n",
    "                        continue\n",
    "                    descr += child.text + ' '\n",
    "                opinion = descr + tok.text\n",
    "        if target:\n",
    "            aspects.append((target, opinion))\n",
    "            asp_doc.append(opinion)\n",
    "    return aspects, asp_doc\n",
    "            \n",
    "aspects, asp_doc = extract(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_neg = asp_doc.paras(categories='neg')\n",
    "rev_pos = asp_doc.paras(categories='pos')\n",
    "#corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref = numpy.array([0] * len(rev_neg) + [1] * len(rev_pos))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(mr, ref)):\n",
    "    x_train, x_test = [asp_doc[indx] for indx in train_index], [asp_doc[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    scores = cross_validate(clf, vectors, ref, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    hyp_polarity = [polarity_doc_level(asp_doc, analyzer) for asp_doc in x_test]\n",
    "    scores_polarity.append(f1_score(y_test, hyp_polarity, average='macro'))\n",
    "\n",
    "    \n",
    "    \n",
    "print('F1 classifier Polarity with Obj:', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('F1 Polarity:',  round(sum(scores_polarity)/len(scores_polarity), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
