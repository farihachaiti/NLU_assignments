{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install en_core_web_sm\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/disi/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     /home/disi/nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/disi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/disi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download(\"subjectivity\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import movie_reviews\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "\n",
    "mr = movie_reviews\n",
    "\n",
    "\n",
    "from nltk.corpus import subjectivity\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer, VaderConstants\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "doc = subjectivity.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol2str(doc):\n",
    "    # flatten & join\n",
    "    return \"\".join([w for sent in doc for w in sent])\n",
    "\n",
    "def score_sent(document, use_pos=False):\n",
    "    pos = []\n",
    "    neg = []\n",
    "    obj = []\n",
    "    for sent in document:\n",
    "        if use_pos:\n",
    "            tagged_sent = pos_tag(sent, tagset='universal')\n",
    "        else:\n",
    "            tagged_sent = [(w, None) for w in sent]\n",
    "\n",
    "        for tok, tag in tagged_sent:\n",
    "            ss = lesk(sent, tok, pos=pos2wn.get(tag, None))\n",
    "            if ss:\n",
    "                sense = swn.senti_synset(ss.name())\n",
    "                pos.append(sense.pos_score())\n",
    "                neg.append(sense.neg_score())\n",
    "                obj.append(sense.obj_score())\n",
    "    return pos, neg, obj\n",
    "\n",
    "def subjectivity_sentence_level(document, analyzer):\n",
    "    S = 0\n",
    "    O = 0\n",
    "    labels = ['S', 'O']\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] != 0:\n",
    "            S+=1\n",
    "        else:\n",
    "            O+=1\n",
    "    return labels[numpy.argmax(numpy.asarray([S, O]))]\n",
    "def rm_objective_sentences(document, analyzer):\n",
    "    new_doc = []\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] != 0:\n",
    "            new_doc.append(\" \".join(sentence))\n",
    "    return new_doc\n",
    "def separate_objective_sentences(document, analyzer):\n",
    "    new_doc = []\n",
    "    obj_doc = []\n",
    "    for sentence in document:\n",
    "        value = analyzer.polarity_scores(\" \".join(sentence))\n",
    "        if value[\"compound\"] != 0:\n",
    "            new_doc.append(\" \".join(sentence))\n",
    "        else:\n",
    "            obj_doc.append(\" \".join(sentence))\n",
    "    return new_doc, obj_doc\n",
    "def polarity_doc_level(document, analyzer):\n",
    "    value = analyzer.polarity_scores(document)\n",
    "    P = 0\n",
    "    N = 0\n",
    "    labels = ['P', 'N']\n",
    "    if value[\"compound\"] > 0:\n",
    "        P += 1\n",
    "    elif value[\"compound\"] <= 0: # In this way we penalize the neg class\n",
    "        N += 1\n",
    "    return labels[numpy.argmax(numpy.asarray([P, N]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Train and test with Stratified K Fold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "scores_clf = []\n",
    "scores_subjectivity = []\n",
    "new_doc = []\n",
    "\n",
    "#corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref_sub, ref_ob = separate_objective_sentences(doc, analyzer)\n",
    "corpus = [lol2str(d) for d in ref_ob] + [lol2str(d) for d in ref_sub]\n",
    "ref = numpy.array([0] * len(ref_ob) + [1] * len(ref_sub))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(corpus, ref)):\n",
    "    x_train, x_test = [corpus[indx] for indx in train_index], [corpus[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "\n",
    "    test_x_split = [[sentence.split() for sentence in corpus] for corpus in x_test]\n",
    "    #[item for sublist in nested_list for item in sublist]\n",
    "  \n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    #scores = cross_validate(clf, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    new_doc.append(hyp)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    #hyp_sentence = [subjectivity_sentence_level(doc, analyzer) for doc in test_x_split]\n",
    "    #scores_subjectivity.append(f1_score(y_test, hyp_sentence, average='macro'))\n",
    "        \n",
    "    \n",
    "print('\\033[1mSubjectivity: F1 classifier:\\033[0m', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('\\033[1mSubjectivity: Classification Report:\\033[0m')\n",
    "print(classification_report(y_test, hyp))\n",
    "#print('F1 Subjectivity:',  round(sum(scores_subjectivity)/len(scores_subjectivity), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf = []\n",
    "\n",
    "\n",
    "rev_neg = mr.paras(categories='neg')\n",
    "rev_pos = mr.paras(categories='pos')\n",
    "\n",
    "rev_neg_wo_objective = [\"\\n\".join(rm_objective_sentences(doc, analyzer)) for doc in rev_neg]\n",
    "rev_pos_wo_objective = [\"\\n\".join(rm_objective_sentences(doc, analyzer)) for doc in rev_pos]\n",
    "\n",
    "corpus_wo_objective = rev_neg_wo_objective + rev_pos_wo_objective\n",
    "#corpus = [lol2str(d) for d in ref_ob] + [lol2str(d) for d in ref_sub]\n",
    "ref = numpy.array([0] * len(rev_neg_wo_objective) + [1] * len(rev_pos_wo_objective))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(corpus_wo_objective, ref)):\n",
    "    x_train, x_test = [corpus_wo_objective[indx] for indx in train_index], [corpus_wo_objective[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    #scores = cross_validate(clf, vectors, labels, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    \n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    #hyp_sentence = [subjectivity_sentence_level(doc, analyzer) for doc in test_x_split]\n",
    "    #scores_subjectivity.append(f1_score(y_test, hyp_sentence, average='macro'))\n",
    "        \n",
    "    \n",
    "print('\\033[1mF1 classifier Polarity w/o Obj:\\033[0m', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('\\033[1mPolarity w/o Obj: Classification Report:\\033[0m')\n",
    "print(classification_report(y_test, hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_clf = []\n",
    "\n",
    "\n",
    "rev_neg = mr.paras(categories='neg')\n",
    "rev_pos = mr.paras(categories='pos')\n",
    "corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref = numpy.array([0] * len(rev_neg) + [1] * len(rev_pos))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(corpus, ref)):\n",
    "    x_train, x_test = [corpus[indx] for indx in train_index], [corpus[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    #scores = cross_validate(clf, vectors, ref, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "    \n",
    "    #hyp_polarity = [polarity_doc_level(doc, analyzer) for doc in x_test]\n",
    "    #scores_polarity.append(f1_score(y_test, hyp_polarity, average='macro'))\n",
    "\n",
    "    \n",
    "    \n",
    "print('\\033[1mF1 classifier Polarity with Obj:\\033[0m', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('\\033[1mPolarity with Obj: Classification Report:\\033[0m')\n",
    "print(classification_report(y_test, hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils import *\n",
    "import numpy\n",
    "import os\n",
    "import re\n",
    "\n",
    "SMALL_POSITIVE_CONST = 1e-4\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        #dataset = f.readlines()\n",
    "    #with open(file_path, 'r') as file:\n",
    "        for line in f:\n",
    "            # Use regular expression to find the sentence before the symbol\n",
    "            match = re.search(r'(.*?){}'.format(re.escape('####')), line)\n",
    "\n",
    "            if match:\n",
    "                sentence_before_symbol = match.group(1).strip()\n",
    "                dataset.append(sentence_before_symbol)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def extract(doc):\n",
    "    aspects = []\n",
    "    for sent in doc.sents:\n",
    "        target = None\n",
    "        opinion = None\n",
    "        for tok in sent:\n",
    "            if tok.dep_ == 'nsubj' and tok.pos_ == 'NOUN':\n",
    "                target = tok.text\n",
    "            if tok.pos_ == 'ADJ':\n",
    "                descr = ''\n",
    "                for child in tok.children:\n",
    "                    if child.pos_ != 'ADV':\n",
    "                        continue\n",
    "                    descr += child.text + ' '\n",
    "                opinion = descr + tok.text\n",
    "        if target:\n",
    "            aspects.append((target, opinion))\n",
    "    return aspects\n",
    "\n",
    "def tag2ot(ote_tag_sequence):\n",
    "    \"\"\"\n",
    "    transform ote tag sequence to a sequence of opinion target\n",
    "    :param ote_tag_sequence: tag sequence for ote task\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_tags = len(ote_tag_sequence)\n",
    "    ot_sequence = []\n",
    "    beg, end = -1, -1\n",
    "    for i in range(n_tags):\n",
    "        tag = ote_tag_sequence[i]\n",
    "        if tag == 'S':\n",
    "            ot_sequence.append((i, i))\n",
    "        elif tag == 'B':\n",
    "            beg = i\n",
    "        elif tag == 'E':\n",
    "            end = i\n",
    "            if end > beg > -1:\n",
    "                ot_sequence.append((beg, end))\n",
    "                beg, end = -1, -1\n",
    "    return ot_sequence\n",
    "\n",
    "def evaluate_ote(gold_ot, pred_ot):\n",
    "    \"\"\"\n",
    "    evaluate the model performce for the ote task\n",
    "    :param gold_ot: gold standard ote tags\n",
    "    :param pred_ot: predicted ote tags\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(gold_ot) == len(pred_ot)\n",
    "    n_samples = len(gold_ot)\n",
    "    # number of true positive, gold standard, predicted opinion targets\n",
    "    n_tp_ot, n_gold_ot, n_pred_ot = 0, 0, 0\n",
    "    #for i in range(n_samples):\n",
    "        #g_ot = gold_ot[i]\n",
    "        #p_ot = pred_ot[i]\n",
    "    g_ot_sequence, p_ot_sequence = tag2ot(ote_tag_sequence=gold_ot), tag2ot(ote_tag_sequence=pred_ot)\n",
    "    # hit number\n",
    "    n_hit_ot = match_ot(gold_ote_sequence=g_ot_sequence, pred_ote_sequence=p_ot_sequence)\n",
    "    n_tp_ot += n_hit_ot\n",
    "    n_gold_ot += len(g_ot_sequence)\n",
    "    n_pred_ot += len(p_ot_sequence)\n",
    "    # add 0.001 for smoothing\n",
    "    # calculate precision, recall and f1 for ote task\n",
    "    ot_precision = float(n_tp_ot) / float(n_pred_ot + SMALL_POSITIVE_CONST)\n",
    "    ot_recall = float(n_tp_ot) / float(n_gold_ot + SMALL_POSITIVE_CONST)\n",
    "    ot_f1 = 2 * ot_precision * ot_recall / (ot_precision + ot_recall + SMALL_POSITIVE_CONST)\n",
    "    ote_scores = (ot_precision, ot_recall, ot_f1)\n",
    "    return ote_scores\n",
    "\n",
    "def tag2ts(ts_tag_sequence):\n",
    "    \"\"\"\n",
    "    transform ts tag sequence to targeted sentiment\n",
    "    :param ts_tag_sequence: tag sequence for ts task\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_tags = len(ts_tag_sequence)\n",
    "    ts_sequence, sentiments = [], []\n",
    "    beg, end = -1, -1\n",
    "    for i in range(n_tags):\n",
    "        ts_tag = ts_tag_sequence[i]\n",
    "        # current position and sentiment\n",
    "        ts_tag = ts_tag.astype(str).tolist()\n",
    "        eles = ts_tag.split('-')\n",
    "        if len(eles) == 2:\n",
    "            pos, sentiment = eles\n",
    "        else:\n",
    "            pos, sentiment = 'O', 'O'\n",
    "        if sentiment != 'O':\n",
    "            # current word is a subjective word\n",
    "            sentiments.append(sentiment)\n",
    "        if pos == 'S':\n",
    "            # singleton\n",
    "            ts_sequence.append((i, i, sentiment))\n",
    "            sentiments = []\n",
    "        elif pos == 'B':\n",
    "            beg = i\n",
    "        elif pos == 'E':\n",
    "            end = i\n",
    "            # schema1: only the consistent sentiment tags are accepted\n",
    "            # that is, all of the sentiment tags are the same\n",
    "            if end > beg > -1 and len(set(sentiments)) == 1:\n",
    "                ts_sequence.append((beg, end, sentiment))\n",
    "                sentiments = []\n",
    "                beg, end = -1, -1\n",
    "    return ts_sequence\n",
    "\n",
    "def evaluate_ts(gold_ts, pred_ts):\n",
    "    \"\"\"\n",
    "    evaluate the model performance for the ts task\n",
    "    :param gold_ts: gold standard ts tags\n",
    "    :param pred_ts: predicted ts tags\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(gold_ts) == len(pred_ts)\n",
    "    n_samples = len(gold_ts)\n",
    "    # number of true postive, gold standard, predicted targeted sentiment\n",
    "    n_tp_ts, n_gold_ts, n_pred_ts = numpy.zeros(3), numpy.zeros(3), numpy.zeros(3)\n",
    "    ts_precision, ts_recall, ts_f1 = numpy.zeros(3), numpy.zeros(3), numpy.zeros(3)\n",
    "\n",
    "    #for i in range(n_samples):\n",
    "        #g_ts = gold_ts[i]\n",
    "        #p_ts = pred_ts[i]\n",
    "    g_ts_sequence, p_ts_sequence = tag2ts(ts_tag_sequence=gold_ts), tag2ts(ts_tag_sequence=pred_ts)\n",
    "    hit_ts_count, gold_ts_count, pred_ts_count = match_ts(gold_ts_sequence=g_ts_sequence,\n",
    "                                                          pred_ts_sequence=p_ts_sequence)\n",
    "\n",
    "    n_tp_ts += hit_ts_count\n",
    "    n_gold_ts += gold_ts_count\n",
    "    n_pred_ts += pred_ts_count\n",
    "        # calculate macro-average scores for ts task\n",
    "    for i in range(3):\n",
    "        n_ts = n_tp_ts[i]\n",
    "        n_g_ts = n_gold_ts[i]\n",
    "        n_p_ts = n_pred_ts[i]\n",
    "        ts_precision[i] = float(n_ts) / float(n_p_ts + SMALL_POSITIVE_CONST)\n",
    "        ts_recall[i] = float(n_ts) / float(n_g_ts + SMALL_POSITIVE_CONST)\n",
    "        ts_f1[i] = 2 * ts_precision[i] * ts_recall[i] / (ts_precision[i] + ts_recall[i] + SMALL_POSITIVE_CONST)\n",
    "\n",
    "    ts_macro_f1 = ts_f1.mean()\n",
    "\n",
    "    # calculate micro-average scores for ts task\n",
    "    n_tp_total = sum(n_tp_ts)\n",
    "    # total sum of TP and FN\n",
    "    n_g_total = sum(n_gold_ts)\n",
    "    # total sum of TP and FP\n",
    "    n_p_total = sum(n_pred_ts)\n",
    "\n",
    "    ts_micro_p = float(n_tp_total) / (n_p_total + SMALL_POSITIVE_CONST)\n",
    "    ts_micro_r = float(n_tp_total) / (n_g_total + SMALL_POSITIVE_CONST)\n",
    "    ts_micro_f1 = 2 * ts_micro_p * ts_micro_r / (ts_micro_p + ts_micro_r + SMALL_POSITIVE_CONST)\n",
    "    ts_scores = (ts_macro_f1, ts_micro_p, ts_micro_r, ts_micro_f1)\n",
    "    return ts_scores\n",
    "\n",
    "\n",
    "def evaluate(gold_ot, gold_ts, pred_ot, pred_ts):\n",
    "    \"\"\"\n",
    "    evaluate the performance of the predictions\n",
    "    :param gold_ot: gold standard opinion target tags\n",
    "    :param gold_ts: gold standard targeted sentiment tags\n",
    "    :param pred_ot: predicted opinion target tags\n",
    "    :param pred_ts: predicted targeted sentiment tags\n",
    "    :return: metric scores of ner and sa\n",
    "    \"\"\"\n",
    "    assert len(gold_ot) == len(gold_ts) == len(pred_ot) == len(pred_ts)\n",
    "    ote_scores = evaluate_ote(gold_ot=gold_ot, pred_ot=pred_ot)\n",
    "    ts_scores = evaluate_ts(gold_ts=gold_ts, pred_ts=pred_ts)\n",
    "    return ote_scores, ts_scores\n",
    "\n",
    "\n",
    "def match_ot(gold_ote_sequence, pred_ote_sequence):\n",
    "    \"\"\"\n",
    "    calculate the number of correctly predicted opinion target\n",
    "    :param gold_ote_sequence: gold standard opinion target sequence\n",
    "    :param pred_ote_sequence: predicted opinion target sequence\n",
    "    :return: matched number\n",
    "    \"\"\"\n",
    "    n_hit = 0\n",
    "    for t in pred_ote_sequence:\n",
    "        if t in gold_ote_sequence:\n",
    "            n_hit += 1\n",
    "    return n_hit\n",
    "\n",
    "\n",
    "def match_ts(gold_ts_sequence, pred_ts_sequence):\n",
    "    \"\"\"\n",
    "    calculate the number of correctly predicted targeted sentiment\n",
    "    :param gold_ts_sequence: gold standard targeted sentiment sequence\n",
    "    :param pred_ts_sequence: predicted targeted sentiment sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # positive, negative and neutral\n",
    "    tag2tagid = {'POS': 0, 'NEG': 1, 'NEU': 2}\n",
    "    hit_count, gold_count, pred_count = numpy.zeros(3), numpy.zeros(3), numpy.zeros(3)\n",
    "    for t in gold_ts_sequence:\n",
    "        #print(t)\n",
    "        ts_tag = t[2]\n",
    "        tid = tag2tagid[ts_tag]\n",
    "        gold_count[tid] += 1\n",
    "    for t in pred_ts_sequence:\n",
    "        ts_tag = t[2]\n",
    "        tid = tag2tagid[ts_tag]\n",
    "        if t in gold_ts_sequence:\n",
    "            hit_count[tid] += 1\n",
    "        pred_count[tid] += 1\n",
    "    return hit_count, gold_count, pred_count\n",
    "\n",
    "def polarity_aspect(aspects, analyzer):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    labels = ['P', 'N']\n",
    "    rev_t_neg = []\n",
    "    rev_t_pos = []\n",
    "    rev_o_neg = []\n",
    "    rev_o_pos = []\n",
    "    \n",
    "    for target, opinion in aspects:\n",
    "        if opinion is None:\n",
    "            opinion = 'Not Related'\n",
    "        else:\n",
    "            value = analyzer.polarity_scores(opinion)\n",
    "            if value[\"compound\"] > 0:\n",
    "                pos += 1 \n",
    "                rev_t_pos.append(target)\n",
    "                rev_o_pos.append(opinion)\n",
    "            elif value[\"compound\"] < 0:\n",
    "                neg += 1\n",
    "                rev_t_neg.append(target)\n",
    "                rev_o_neg.append(opinion)\n",
    "    return rev_t_pos, rev_o_pos, rev_t_neg, rev_o_neg\n",
    "\n",
    "def polarity_doc_level(document, analyzer):\n",
    "    rev_neg = []\n",
    "    rev_pos = []\n",
    "    value = analyzer.polarity_scores(document)\n",
    "    if value[\"compound\"] > 0:\n",
    "        rev_pos.append(sentence)\n",
    "    elif value[\"compound\"] <= 0: # In this way we penalize the neg class\n",
    "        rev_neg.append(sentence) \n",
    "    return rev_neg, rev_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = load_data(os.path.join('dataset/laptop14_train.txt'))\n",
    "test_raw = load_data(os.path.join('dataset/laptop14_test.txt'))\n",
    "#print(nlp(' '.join(train_raw)))\n",
    "aspects_train = extract(nlp(' '.join(train_raw)))\n",
    "\n",
    "aspects_test = extract(nlp(' '.join(test_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mF1 classifier Polarity of aspect terms:\u001b[0m 0.476\n",
      "\u001b[1mPolarity of Aspect terms: Classification Report\u001b[0m:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.54      0.88      0.67         8\n",
      "           2       1.00      1.00      1.00         1\n",
      "           3       1.00      0.25      0.40         8\n",
      "\n",
      "    accuracy                           0.61        18\n",
      "   macro avg       0.76      0.78      0.68        18\n",
      "weighted avg       0.77      0.61      0.57        18\n",
      "\n",
      "18\n",
      "18\n",
      "\u001b[1mSEMEVAL Polarity of aspect terms:\u001b[0m\n",
      "((0.0, 0.0, 0.0), (0.0, 0.0, 0.0, 0.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/disi/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-3348684135e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\033[1mSEMEVAL Polarity of aspect terms:\\033[0m'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_targ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_targ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-1efaed4db2fc>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(gold_ot, gold_ts, pred_ot, pred_ts)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0mscores\u001b[0m \u001b[0mof\u001b[0m \u001b[0mner\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_ot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_ts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ot\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mote_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_ote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_ot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgold_ot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mts_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_ts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold_ts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgold_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_ts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores_clf = []\n",
    "hyp_targ = []\n",
    "ref_targ = []\n",
    "hyp_op = []\n",
    "ref_op = []\n",
    "\n",
    "rev_t_pos, rev_o_pos, rev_t_neg, rev_o_neg = polarity_aspect(aspects_train, analyzer)\n",
    "corpus_trn = [lol2str(d) for d in rev_t_neg] + [lol2str(d) for d in rev_t_pos] + [lol2str(d) for d in rev_o_neg] + [lol2str(d) for d in rev_o_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "ref_trn = numpy.array([0] * len(rev_t_neg) + [1] * len(rev_t_pos) + [2] * len(rev_o_neg) + [3] * len(rev_o_pos))\n",
    "\n",
    "rev_t_pos2, rev_o_pos2, rev_t_neg2, rev_o_neg2 = polarity_aspect(aspects_test, analyzer)\n",
    "corpus_tst = [lol2str(d) for d in rev_t_neg2] + [lol2str(d) for d in rev_t_pos2] + [lol2str(d) for d in rev_o_neg2] + [lol2str(d) for d in rev_o_pos2]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref_tst = numpy.array([0] * len(rev_t_neg2) + [1] * len(rev_t_pos2) + [2] * len(rev_o_neg2) + [3] * len(rev_o_pos2))\n",
    "\n",
    "#for (train_index, test_index) in enumerate(skf.split(corpus, ref)):\n",
    "#for i, (train_index, test_index) in enumerate(skf.split(corpus_trn, ref_trn), skf.split(corpus_tst, ref_tst)):\n",
    "for i, ((train_index, train_index), (test_index, test_index)) in enumerate(zip(skf.split(corpus_trn, ref_trn), skf.split(corpus_tst, ref_tst))):\n",
    "\n",
    "    x_train, x_test = [corpus_trn[idx] for idx in train_index], [corpus_tst[idx] for idx in test_index]\n",
    "    y_train, y_test = [ref_trn[idx] for idx in train_index], [ref_tst[idx] for idx in test_index]\n",
    "\n",
    "    # Needed for word and sentence level\n",
    "    #test_x_split = [[sentence for sentence in doc] for doc in x_test]\n",
    "\n",
    "    #x_train = ' '.join(x_train[0])\n",
    "    #x_test = [' '.join(doc) for doc in x_test]\n",
    "    #unequal_indices = numpy.where(x_train != y_train)\n",
    "\n",
    "    # Increase elements in array1 at unequal indices\n",
    "    #array1[unequal_indices] += 1\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    train_features = vectorizer.transform(x_train)\n",
    "    test_features = vectorizer.transform(x_test)\n",
    "\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300).fit(train_features, y_train)\n",
    "    #scores = cross_validate(clf, vectors, ref, cv=StratifiedKFold(n_splits=10) , scoring=['f1_micro'])\n",
    "    hyp = clf.predict(test_features)\n",
    "    scores_clf.append(f1_score(y_test, hyp, average='macro'))\n",
    "\n",
    "    #hyp_polarity = [polarity_doc_level(doc, analyzer) for doc in asp_doc_test]\n",
    "    #scores_polarity.append(f1_score(y_test, hyp_polarity, average='macro'))\n",
    "\n",
    "    \n",
    "print('\\033[1mF1 classifier Polarity of aspect terms:\\033[0m', round(sum(scores_clf)/len(scores_clf), 3))\n",
    "print('\\033[1mPolarity of Aspect terms: Classification Report\\033[0m:')\n",
    "print(classification_report(y_test, hyp))\n",
    "print(len(hyp))\n",
    "print(len(y_test))\n",
    "for i in hyp:\n",
    "    if i==1:\n",
    "        hyp_targ.append(i)\n",
    "        ref_targ.append(i)\n",
    "    else:\n",
    "        hyp_op.append(i)\n",
    "        ref_op.append(i)\n",
    "print('\\033[1mSEMEVAL Polarity of aspect terms:\\033[0m')\n",
    "print(evaluate(y_test, y_test, hyp, hyp))\n",
    "print(evaluate(ref_op, ref_targ, hyp_op, hyp_targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"The setting of this hotel is really lovely. \n",
    "Rooms are spacious, spotlessly clean, with large and comfortable beds. \n",
    "We appreciated the courtesy touches, such as the fresh fruit on the table, always replaced.\n",
    "\n",
    "However, our room, in the new building, came at 200 euros a night (admittedly it was a busy weekend), and at this price one begins of course to be more demanding and less forgiving :-) , so let me mention that I would expect a better shower, and that the parking and manoeuvrability situation is not the best (narrow passage + slight chaos when full).\n",
    "\n",
    "But I want to conclude on a positive note: their breakfast is very good, with a sense of abundance, fresh juices, many jams and teas and cereals and nuts, proper just prepared scrambled eggs, (note for Anglo-Saxon visitors: it's mostly \"European\" style, so apart from the eggs don't expect many other hot/cooked items). And service (as well as reception) is excellent, with some people having more natural human warmth than others, but always courteous and helpful.\"\"\"\n",
    "doc = nlp(txt)\n",
    "print(doc)\n",
    "for sent in doc.sents:\n",
    "    for tok in sent:\n",
    "        if tok.dep_ == 'nsubj' and tok.pos_ == 'NOUN':\n",
    "            print(tok.text)\n",
    "aspects = extract(doc)\n",
    "print(aspects)\n",
    "#print(train_raw)\n",
    "aspects = extract(nlp(' '.join(train_raw)))\n",
    "print(aspects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp(' '.join(train_raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "scores_clf = []\n",
    "scores_polarity = []\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "rev_neg = mr.paras(categories='neg')\n",
    "rev_pos = mr.paras(categories='pos')\n",
    "corpus = [lol2str(d) for d in rev_neg] + [lol2str(d) for d in rev_pos]\n",
    "#vectors = vectorizer.fit_transform(mr)\n",
    "\n",
    "ref = numpy.array([0] * len(rev_neg) + [1] * len(rev_pos))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(corpus, ref)):\n",
    "    x_train, x_test = [corpus[indx] for indx in train_index], [corpus[indx] for indx in test_index]\n",
    "    y_train, y_test = [ref[indx] for indx in train_index], [ref[indx] for indx in test_index]\n",
    "    # Needed for word and sentence level\n",
    "    test_x_split = [[sentence.split() for sentence in doc.splitlines()] for doc in x_test]\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(x_train)\n",
    "    print(len(x_train))\n",
    "    print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
