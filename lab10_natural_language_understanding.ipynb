{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDCAZtySMC-7",
    "outputId": "45c4a581-2127-4f8b-cc50-00e9148d39a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 4978\n",
      "Test samples: 893\n",
      "{'intent': 'flight',\n",
      " 'slots': 'O O O O O O O O B-fromloc.city_name O B-toloc.city_name '\n",
      "          'I-toloc.city_name O O O O O B-stoploc.city_name I-stoploc.city_name',\n",
      " 'utterance': 'i would like to find a flight from charlotte to las vegas that '\n",
      "              'makes a stop in st. louis'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "# Global variables\n",
    "import os\n",
    "device = 'cuda:0' # cuda:0 means we are using the GPU with id 0, if you have multiple GPU\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # Used to report errors on CUDA side\n",
    "PAD_TOKEN = 0\n",
    "from collections import Counter\n",
    "\n",
    "def load_data(path):\n",
    "    '''\n",
    "        input: path/to/data\n",
    "        output: json\n",
    "    '''\n",
    "    dataset = []\n",
    "    with open(path) as f:\n",
    "        dataset = json.loads(f.read())\n",
    "    return dataset\n",
    "\n",
    "tmp_train_raw = load_data(os.path.join('dataset/ATIS/train.json'))\n",
    "test_raw = load_data(os.path.join('dataset/ATIS/test.json'))\n",
    "print('Train samples:', len(tmp_train_raw))\n",
    "print('Test samples:', len(test_raw))\n",
    "\n",
    "pprint(test_raw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NA0oJlHTMC-9",
    "outputId": "62a66379-f7e7-4218-de02-cb7dc96cd925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "{'abbreviation': 2.9000000000000004,\n",
      " 'aircraft': 1.6,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airline+flight_no': 0.0,\n",
      " 'airport': 0.4,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.4,\n",
      " 'distance': 0.4,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.4,\n",
      " 'flight_no': 0.3,\n",
      " 'flight_time': 1.0999999999999999,\n",
      " 'ground_fare': 0.4,\n",
      " 'ground_service': 5.1,\n",
      " 'meal': 0.1,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.1}\n",
      "Dev:\n",
      "{'abbreviation': 3.0,\n",
      " 'aircraft': 1.7000000000000002,\n",
      " 'airfare': 8.5,\n",
      " 'airline': 3.2,\n",
      " 'airport': 0.3,\n",
      " 'capacity': 0.3,\n",
      " 'city': 0.3,\n",
      " 'distance': 0.3,\n",
      " 'flight': 73.7,\n",
      " 'flight+airfare': 0.5,\n",
      " 'flight_no': 0.2,\n",
      " 'flight_time': 1.0,\n",
      " 'ground_fare': 0.3,\n",
      " 'ground_service': 5.2,\n",
      " 'meal': 0.2,\n",
      " 'quantity': 1.0,\n",
      " 'restriction': 0.2}\n",
      "Test:\n",
      "{'abbreviation': 3.6999999999999997,\n",
      " 'aircraft': 1.0,\n",
      " 'airfare': 5.4,\n",
      " 'airfare+flight': 0.1,\n",
      " 'airline': 4.3,\n",
      " 'airport': 2.0,\n",
      " 'capacity': 2.4,\n",
      " 'city': 0.7000000000000001,\n",
      " 'day_name': 0.2,\n",
      " 'distance': 1.0999999999999999,\n",
      " 'flight': 70.8,\n",
      " 'flight+airfare': 1.3,\n",
      " 'flight+airline': 0.1,\n",
      " 'flight_no': 0.8999999999999999,\n",
      " 'flight_no+airline': 0.1,\n",
      " 'flight_time': 0.1,\n",
      " 'ground_fare': 0.8,\n",
      " 'ground_service': 4.0,\n",
      " 'meal': 0.7000000000000001,\n",
      " 'quantity': 0.3}\n",
      "=========================================================================================\n",
      "TRAIN size: 4381\n",
      "DEV size: 597\n",
      "TEST size: 893\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# Firt we get the 10% of dataset, then we compute the percentage of these examples\n",
    "# on the training set which is around 11%\n",
    "portion = round(((len(tmp_train_raw) + len(test_raw)) * 0.10)/(len(tmp_train_raw)),2)\n",
    "\n",
    "\n",
    "intents = [x['intent'] for x in tmp_train_raw] # We stratify on intents\n",
    "count_y = Counter(intents)\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "mini_Train = []\n",
    "\n",
    "for id_y, y in enumerate(intents):\n",
    "    if count_y[y] > 1: # If some intents occure once only, we put them in training\n",
    "        X.append(tmp_train_raw[id_y])\n",
    "        Y.append(y)\n",
    "    else:\n",
    "        mini_Train.append(tmp_train_raw[id_y])\n",
    "# Random Stratify\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, Y, test_size=portion,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=Y)\n",
    "X_train.extend(mini_Train)\n",
    "train_raw = X_train\n",
    "dev_raw = X_dev\n",
    "\n",
    "y_test = [x['intent'] for x in test_raw]\n",
    "\n",
    "# Intent distribution\n",
    "print('Train:')\n",
    "pprint({k:round(v/len(y_train),3)*100 for k, v in sorted(Counter(y_train).items())})\n",
    "print('Dev:'),\n",
    "pprint({k:round(v/len(y_dev),3)*100 for k, v in sorted(Counter(y_dev).items())})\n",
    "print('Test:')\n",
    "pprint({k:round(v/len(y_test),3)*100 for k, v in sorted(Counter(y_test).items())})\n",
    "print('='*89)\n",
    "# Dataset size\n",
    "print('TRAIN size:', len(train_raw))\n",
    "print('DEV size:', len(dev_raw))\n",
    "print('TEST size:', len(test_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MZoTYkcdMC--"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Lang():\n",
    "    def __init__(self, words, intents, slots, cutoff=0):\n",
    "        self.word2id = self.w2id(words, cutoff=cutoff, unk=True)\n",
    "        self.slot2id = self.lab2id(slots)\n",
    "        self.intent2id = self.lab2id(intents, pad=False)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        self.id2slot = {v:k for k, v in self.slot2id.items()}\n",
    "        self.id2intent = {v:k for k, v in self.intent2id.items()}\n",
    "\n",
    "    def w2id(self, elements, cutoff=None, unk=True):\n",
    "        vocab = {'pad': PAD_TOKEN}\n",
    "        if unk:\n",
    "            vocab['unk'] = len(vocab)\n",
    "        count = Counter(elements)\n",
    "        for k, v in count.items():\n",
    "            if v > cutoff:\n",
    "                vocab[k] = len(vocab)\n",
    "        return vocab\n",
    "\n",
    "    def lab2id(self, elements, pad=True):\n",
    "        vocab = {}\n",
    "        if pad:\n",
    "            vocab['pad'] = PAD_TOKEN\n",
    "        for elem in elements:\n",
    "                vocab[elem] = len(vocab)\n",
    "        return vocab\n",
    "words = sum([x['utterance'].split() for x in train_raw], []) # No set() since we want to compute\n",
    "                                                            # the cutoff\n",
    "corpus = train_raw + dev_raw + test_raw # We do not wat unk labels,\n",
    "                                        # however this depends on the research purpose\n",
    "slots = set(sum([line['slots'].split() for line in corpus],[]))\n",
    "intents = set([line['intent'] for line in corpus])\n",
    "\n",
    "lang = Lang(words, intents, slots, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T222UVjbMC-_",
    "outputId": "62ca20a5-9d09-4f97-fcd7-89ebaf660914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/disi/.local/lib/python3.8/site-packages (0.0.post5)\n",
      "Requirement already satisfied: scikit-learn in /home/disi/.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/disi/.local/lib/python3.8/site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/disi/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/disi/.local/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/disi/.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/disi/.local/lib/python3.8/site-packages (4.65.0)\n",
      "Requirement already satisfied: transformers in /home/disi/.local/lib/python3.8/site-packages (4.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: filelock in /home/disi/.local/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/disi/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/disi/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /home/disi/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEBIKMEOMC-_",
    "outputId": "62175fd5-121c-42f5-b3b5-99c73a2b2450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Vocab: 861\n",
      "# Slots: 120\n",
      "# Intent: 22\n",
      "# Vocab: 861\n",
      "# Slots: 123\n",
      "# Intent: 22\n",
      "# Vocab: 861\n",
      "# Slots: 129\n",
      "# Intent: 26\n"
     ]
    }
   ],
   "source": [
    "w2id = {'pad':PAD_TOKEN, 'unk': 1}\n",
    "slot2id = {'pad':PAD_TOKEN}\n",
    "intent2id = {}\n",
    "# Map the words only from the train set\n",
    "# Map slot and intent labels of train, dev and test set. 'unk' is not needed.\n",
    "for example in train_raw:\n",
    "    for w in example['utterance'].split():\n",
    "        if w not in w2id:\n",
    "            w2id[w] = len(w2id)\n",
    "    for slot in example['slots'].split():\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    if example['intent'] not in intent2id:\n",
    "        intent2id[example['intent']] = len(intent2id)\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))\n",
    "for example in dev_raw:\n",
    "    for slot in example['slots'].split():\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    if example['intent'] not in intent2id:\n",
    "        intent2id[example['intent']] = len(intent2id)\n",
    "\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))\n",
    "\n",
    "for example in test_raw:\n",
    "    for slot in example['slots'].split():\n",
    "        if slot not in slot2id:\n",
    "            slot2id[slot] = len(slot2id)\n",
    "    if example['intent'] not in intent2id:\n",
    "        intent2id[example['intent']] = len(intent2id)\n",
    "\n",
    "print('# Vocab:', len(w2id)-2) # we remove pad and unk from the count\n",
    "print('# Slots:', len(slot2id)-1)\n",
    "print('# Intent:', len(intent2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m11w0uGVMC_A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class IntentsAndSlots (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, dataset, lang, unk='unk'):\n",
    "        self.utterances = []\n",
    "        self.intents = []\n",
    "        self.slots = []\n",
    "        self.unk = unk\n",
    "\n",
    "        for x in dataset:\n",
    "            self.utterances.append(x['utterance'])\n",
    "            self.slots.append(x['slots'])\n",
    "            self.intents.append(x['intent'])\n",
    "\n",
    "        self.utt_ids = self.mapping_seq(self.utterances, lang.word2id)\n",
    "        self.slot_ids = self.mapping_seq(self.slots, lang.slot2id)\n",
    "        self.intent_ids = self.mapping_lab(self.intents, lang.intent2id)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.utterances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        utt = torch.Tensor(self.utt_ids[idx])\n",
    "        slots = torch.Tensor(self.slot_ids[idx])\n",
    "        intent = self.intent_ids[idx]\n",
    "        sample = {'utterance': utt, 'slots': slots, 'intent': intent}\n",
    "        return sample\n",
    "\n",
    "    # Auxiliary methods\n",
    "\n",
    "    def mapping_lab(self, data, mapper):\n",
    "        return [mapper[x] if x in mapper else mapper[self.unk] for x in data]\n",
    "\n",
    "    def mapping_seq(self, data, mapper): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq.split():\n",
    "                if x in mapper:\n",
    "                    tmp_seq.append(mapper[x])\n",
    "                else:\n",
    "                    tmp_seq.append(mapper[self.unk])\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "# Create our datasets\n",
    "train_dataset = IntentsAndSlots(train_raw, lang)\n",
    "dev_dataset = IntentsAndSlots(dev_raw, lang)\n",
    "test_dataset = IntentsAndSlots(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WpSjNp6fMC_A"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(data):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len\n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape\n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(PAD_TOKEN)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        # print(padded_seqs)\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "    data.sort(key=lambda x: len(x['utterance']), reverse=True)\n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "    # We just need one length for packed pad seq, since len(utt) == len(slots)\n",
    "    src_utt, _ = merge(new_item['utterance'])\n",
    "    y_slots, y_lengths = merge(new_item[\"slots\"])\n",
    "    intent = torch.LongTensor(new_item[\"intent\"])\n",
    "\n",
    "    src_utt = src_utt.to(device) # We load the Tensor on our seleceted device\n",
    "    y_slots = y_slots.to(device)\n",
    "    intent = intent.to(device)\n",
    "    y_lengths = torch.LongTensor(y_lengths).to(device)\n",
    "\n",
    "    new_item[\"utterances\"] = src_utt\n",
    "    new_item[\"intents\"] = intent\n",
    "    new_item[\"y_slots\"] = y_slots\n",
    "    new_item[\"slots_len\"] = y_lengths\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=collate_fn,  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class ModelIAS(nn.Module):\n",
    "\n",
    "    def __init__(self, hid_size, out_slot, out_int, emb_size, vocab_len, n_layer=2, pad_index=0):\n",
    "        super(ModelIAS, self).__init__()\n",
    "        # hid_size = Hidden size\n",
    "        # out_slot = number of slots (output size for slot filling)\n",
    "        # out_int = number of intents (ouput size for intent class)\n",
    "        # emb_size = word embedding size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_size, padding_idx=pad_index)\n",
    "\n",
    "        self.utt_encoder = nn.LSTM(emb_size, hid_size, n_layer, bidirectional=True)\n",
    "        self.slot_out = nn.Linear(hid_size*2, out_slot)\n",
    "        self.intent_out = nn.Linear(hid_size, out_int)\n",
    "        # Dropout layer How do we apply it?\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, utterance, seq_lengths):\n",
    "        # utterance.size() = batch_size X seq_len\n",
    "        utt_emb = self.embedding(utterance) # utt_emb.size() = batch_size X seq_len X emb_size\n",
    "        utt_emb = utt_emb.permute(1,0,2) # we need seq len first -> seq_len X batch_size X emb_size\n",
    "\n",
    "        # pack_padded_sequence avoid computation over pad tokens reducing the computational cost\n",
    "\n",
    "        packed_input = pack_padded_sequence(utt_emb, seq_lengths.cpu().numpy())\n",
    "        # Process the batch\n",
    "        packed_output, (last_hidden, cell) = self.utt_encoder(packed_input)\n",
    "        # Unpack the sequence\n",
    "        utt_encoded, input_sizes = pad_packed_sequence(packed_output)\n",
    "        # Get the last hidden state\n",
    "        last_hidden = last_hidden[-1,:,:]\n",
    "        # Compute slot logits\n",
    "        slots = self.slot_out(utt_encoded)\n",
    "        slots = self.dropout(slots)\n",
    "        # Compute intent logits\n",
    "        intent = self.intent_out(last_hidden)\n",
    "        intent = self.dropout(intent)\n",
    "\n",
    "        # Slot size: seq_len, batch size, calsses\n",
    "        slots = slots.permute(1,2,0) # We need this for computing the loss\n",
    "        # Slot size: batch_size, classes, seq_len\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "lr = 0.0001 # learning rate\n",
    "clip = 5 # Clip the gradient\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(hid_size, out_slot, out_int, emb_size, vocab_len, pad_index=PAD_TOKEN).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion_slots = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "criterion_intents = nn.CrossEntropyLoss() # Because we do not have the pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_loop(data, optimizer, criterion_slots, critenrion_intents, model):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        slots, intent = model(sample['utterances'], sample['slots_len'])\n",
    "        loss_intent = criterion_intents(intent, sample['intents'])\n",
    "        loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "        loss = loss_intent + loss_slot # In joint training we sum the losses.\n",
    "                                       # Is there another way to do that?\n",
    "        loss_array.append(loss.item())\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step() # Update the weights\n",
    "    return loss_array\n",
    "\n",
    "def eval_loop(data, criterion_slots, criterion_intents, model, lang):\n",
    "    model.eval()\n",
    "    loss_array = []\n",
    "\n",
    "    ref_intents = []\n",
    "    hyp_intents = []\n",
    "\n",
    "    ref_slots = []\n",
    "    hyp_slots = []\n",
    "    #softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            slots, intents = model(sample['utterances'], sample['slots_len'])\n",
    "            loss_intent = criterion_intents(intents, sample['intents'])\n",
    "            loss_slot = criterion_slots(slots, sample['y_slots'])\n",
    "            loss = loss_intent + loss_slot\n",
    "            loss_array.append(loss.item())\n",
    "            # Intent inference\n",
    "            # Get the highest probable class\n",
    "            out_intents = [lang.id2intent[x]\n",
    "                           for x in torch.argmax(intents, dim=1).tolist()]\n",
    "            gt_intents = [lang.id2intent[x] for x in sample['intents'].tolist()]\n",
    "            ref_intents.extend(gt_intents)\n",
    "            hyp_intents.extend(out_intents)\n",
    "\n",
    "            # Slot inference\n",
    "            output_slots = torch.argmax(slots, dim=1)\n",
    "            for id_seq, seq in enumerate(output_slots):\n",
    "                length = sample['slots_len'].tolist()[id_seq]\n",
    "                utt_ids = sample['utterance'][id_seq][:length].tolist()\n",
    "                gt_ids = sample['y_slots'][id_seq].tolist()\n",
    "                gt_slots = [lang.id2slot[elem] for elem in gt_ids[:length]]\n",
    "                utterance = [lang.id2word[elem] for elem in utt_ids]\n",
    "                to_decode = seq[:length].tolist()\n",
    "                ref_slots.append([(utterance[id_el], elem) for id_el, elem in enumerate(gt_slots)])\n",
    "                tmp_seq = []\n",
    "                for id_el, elem in enumerate(to_decode):\n",
    "                    tmp_seq.append((utterance[id_el], lang.id2slot[elem]))\n",
    "                hyp_slots.append(tmp_seq)\n",
    "    try:\n",
    "        results = evaluate(ref_slots, hyp_slots)\n",
    "    except Exception as ex:\n",
    "        # Sometimes the model predics a class that is not in REF\n",
    "        print(ex)\n",
    "        ref_s = set([x[1] for x in ref_slots])\n",
    "        hyp_s = set([x[1] for x in hyp_slots])\n",
    "        print(hyp_s.difference(ref_s))\n",
    "\n",
    "    report_intent = classification_report(ref_intents, hyp_intents,\n",
    "                                          zero_division=False, output_dict=True)\n",
    "    return results, report_intent, loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/199 [00:00<?, ?it/s]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "  3%|▎         | 5/199 [00:05<03:34,  1.11s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "  5%|▌         | 10/199 [00:10<03:18,  1.05s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "  8%|▊         | 15/199 [00:15<03:11,  1.04s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 10%|█         | 20/199 [00:20<03:04,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 13%|█▎        | 25/199 [00:25<02:59,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 15%|█▌        | 30/199 [00:31<02:55,  1.04s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 18%|█▊        | 35/199 [00:36<02:48,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 20%|██        | 40/199 [00:41<02:44,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 23%|██▎       | 45/199 [00:46<02:38,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 25%|██▌       | 50/199 [00:51<02:34,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 28%|██▊       | 55/199 [00:56<02:28,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 30%|███       | 60/199 [01:01<02:25,  1.04s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 33%|███▎      | 65/199 [01:06<02:18,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 35%|███▌      | 70/199 [01:11<02:13,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 38%|███▊      | 75/199 [01:16<02:07,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 40%|████      | 80/199 [01:21<02:02,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 43%|████▎     | 85/199 [01:26<01:57,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 45%|████▌     | 90/199 [01:31<01:52,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 48%|████▊     | 95/199 [01:36<01:47,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 50%|█████     | 100/199 [01:41<01:44,  1.06s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 53%|█████▎    | 105/199 [01:46<01:37,  1.04s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 55%|█████▌    | 110/199 [01:51<01:32,  1.04s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 58%|█████▊    | 115/199 [01:56<01:26,  1.04s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 60%|██████    | 120/199 [02:01<01:22,  1.05s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 63%|██████▎   | 125/199 [02:06<01:16,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 65%|██████▌   | 130/199 [02:11<01:11,  1.03s/it]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      " 67%|██████▋   | 134/199 [02:16<01:06,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mResults of Modified model MODELIAS:\u001b[0m\n",
      "Slot F1:  0.9356435643564356\n",
      "Intent Accuracy: 0.9507278835386338\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "n_epochs = 200\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_f1 = 0\n",
    "for x in tqdm(range(1,n_epochs)):\n",
    "    loss = train_loop(train_loader, optimizer, criterion_slots,\n",
    "                      criterion_intents, model)\n",
    "    if x % 5 == 0:\n",
    "        sampled_epochs.append(x)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        results_dev, intent_res, loss_dev = eval_loop(dev_loader, criterion_slots,\n",
    "                                                      criterion_intents, model, lang)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        f1 = results_dev['total']['f']\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "\n",
    "results_test, intent_test, _ = eval_loop(test_loader, criterion_slots,\n",
    "                                         criterion_intents, model, lang)\n",
    "print(\"\\033[1mResults of Modified model MODELIAS:\\033[0m\")\n",
    "print('Slot F1: ', results_test['total']['f'])\n",
    "print('Intent Accuracy:', intent_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPpxZyX3MC_D",
    "outputId": "5e986ac8-668a-4269-a929-ec757799f66f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: ipywidgets in /home/disi/.local/lib/python3.8/site-packages (8.1.1)\n",
      "Requirement already satisfied, skipping upgrade: comm>=0.1.3 in /home/disi/.local/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython>=6.1.0 in /usr/lib/python3/dist-packages (from ipywidgets) (7.13.0)\n",
      "Requirement already satisfied, skipping upgrade: jupyterlab-widgets~=3.0.9 in /home/disi/.local/lib/python3.8/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied, skipping upgrade: widgetsnbextension~=4.0.9 in /home/disi/.local/lib/python3.8/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied, skipping upgrade: pexpect in /usr/lib/python3/dist-packages (from ipython>=6.1.0->ipywidgets) (4.6.0)\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "1PVBLv3NMC_D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "#from conll import evaluate\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import default_data_collator, TFBertModel\n",
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from tensorflow.keras.layers import Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class ModelIAS(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, max_len, total_intent_no=None, total_slot_no=None, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.total_intent_no = total_intent_no\n",
    "        self.total_slot_no = total_slot_no\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.slot_out = Dense(self.total_slot_no, activation='softmax')\n",
    "        self.intent_out = Dense(self.total_intent_no, activation='softmax')\n",
    "        # Dropout layer How do we apply it?\n",
    "        self.dropout = Dropout(self.dropout_prob)\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self, tokenizer, text_sequence, max_length):\n",
    "        encoded = tokenizer(text_sequence, return_tensors='pt', is_split_into_words=True)\n",
    "        input_ids = encoded['input_ids'].unsqueeze(0)\n",
    "        attention_mask = encoded['attention_mask'].unsqueeze(0)\n",
    "        token_type_ids = encoded['token_type_ids'].unsqueeze(0)\n",
    "        return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.model(inputs)\n",
    "        slots = self.dropout(outputs[0])\n",
    "        slots = self.slot_out(slots)\n",
    "        intent = self.dropout(outputs[1])\n",
    "        intent = self.intent_out(intent)\n",
    "\n",
    "        return slots, intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRTAvBSAMC_E",
    "outputId": "67d38eb2-841a-4f47-c872-4d35417e2531",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1/1 [==============================] - 31s 31s/step - loss: 9.1926 - output_1_loss: 4.3289 - output_2_loss: 4.8637 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - val_loss: 6.6519 - val_output_1_loss: 4.8115 - val_output_2_loss: 1.8404 - val_output_1_accuracy: 0.0000e+00 - val_output_2_accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 5.4635 - output_1_loss: 2.8467 - output_2_loss: 2.6167 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - val_loss: 6.8957 - val_output_1_loss: 4.9304 - val_output_2_loss: 1.9653 - val_output_1_accuracy: 0.0000e+00 - val_output_2_accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 8.7404 - output_1_loss: 6.1689 - output_2_loss: 2.5715 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - val_loss: 6.5873 - val_output_1_loss: 4.7773 - val_output_2_loss: 1.8100 - val_output_1_accuracy: 0.0000e+00 - val_output_2_accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 6.6523 - output_1_loss: 4.4546 - output_2_loss: 2.1977 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 1.0000 - val_loss: 5.9758 - val_output_1_loss: 4.3667 - val_output_2_loss: 1.6091 - val_output_1_accuracy: 0.0000e+00 - val_output_2_accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 5.3729 - output_1_loss: 4.3381 - output_2_loss: 1.0348 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 1.0000 - val_loss: 5.5661 - val_output_1_loss: 4.0500 - val_output_2_loss: 1.5162 - val_output_1_accuracy: 0.0000e+00 - val_output_2_accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 6.3750 - output_1_loss: 4.2993 - output_2_loss: 2.0758 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - val_loss: 4.9104 - val_output_1_loss: 3.5409 - val_output_2_loss: 1.3694 - val_output_1_accuracy: 0.0000e+00 - val_output_2_accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 5.6429 - output_1_loss: 4.1722 - output_2_loss: 1.4707 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 1.0000 - val_loss: 4.1414 - val_output_1_loss: 2.9270 - val_output_2_loss: 1.2144 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 3.3787 - output_1_loss: 2.3588 - output_2_loss: 1.0200 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 3.5113 - val_output_1_loss: 2.4147 - val_output_2_loss: 1.0965 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "1/1 [==============================] - 0s 422ms/step - loss: 4.3405 - output_1_loss: 2.7373 - output_2_loss: 1.6032 - output_1_accuracy: 0.0000e+00 - output_2_accuracy: 1.0000 - val_loss: 2.8242 - val_output_1_loss: 1.8648 - val_output_2_loss: 0.9594 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 3.8145 - output_1_loss: 2.2128 - output_2_loss: 1.6018 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 2.3026 - val_output_1_loss: 1.4451 - val_output_2_loss: 0.8575 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 2.5013 - output_1_loss: 1.4415 - output_2_loss: 1.0598 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 1.7992 - val_output_1_loss: 1.0350 - val_output_2_loss: 0.7642 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 2.3562 - output_1_loss: 1.4469 - output_2_loss: 0.9093 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 1.3670 - val_output_1_loss: 0.6846 - val_output_2_loss: 0.6824 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 3.0075 - output_1_loss: 2.2311 - output_2_loss: 0.7764 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 1.0529 - val_output_1_loss: 0.4399 - val_output_2_loss: 0.6131 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 2.8688 - output_1_loss: 2.1228 - output_2_loss: 0.7461 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 0.8528 - val_output_1_loss: 0.2948 - val_output_2_loss: 0.5580 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 1.0034 - output_1_loss: 0.5554 - output_2_loss: 0.4479 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000 - val_loss: 0.7094 - val_output_1_loss: 0.1992 - val_output_2_loss: 0.5102 - val_output_1_accuracy: 1.0000 - val_output_2_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7094 - output_1_loss: 0.1992 - output_2_loss: 0.5102 - output_1_accuracy: 1.0000 - output_2_accuracy: 1.0000\n",
      "\u001b[1mResults of BERT MODEL:\u001b[0m\n",
      "Loss: 0.7094132900238037\n",
      "Slot Loss: 0.19924859702587128\n",
      "Intent Loss: 0.5101646780967712\n",
      "Slot Accuracy: 1.0\n",
      "Intent Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "lr = 3e-5 # learning rate\n",
    "e = 1e-08\n",
    "\n",
    "out_slot = len(lang.slot2id)\n",
    "out_int = len(lang.intent2id)\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = ModelIAS(vocab_len, out_int, out_slot)\n",
    "\n",
    "optimizer = Adam(learning_rate=lr, epsilon=e)\n",
    "losses = [SparseCategoricalCrossentropy(from_logits=False),\n",
    "          SparseCategoricalCrossentropy(from_logits=False)]\n",
    "metrics = [SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=losses, metrics=metrics)\n",
    "\n",
    "\n",
    "def encode_dataset(tokenizer, text_sequences, max_length):\n",
    "    token_ids = np.zeros(shape=(len(text_sequences), max_length),\n",
    "                         dtype=np.int32)\n",
    "    for i, text_sequence in enumerate(text_sequences):\n",
    "        encoded = tokenizer.encode(text_sequence)\n",
    "        token_ids[i, 0:len(encoded)] = encoded\n",
    "    attention_masks = (token_ids != 0).astype(np.int32)\n",
    "    return {\"input_ids\": token_ids, \"attention_masks\": attention_masks}\n",
    "\n",
    "\n",
    "def prepare_dataset(data, model):\n",
    "    input_ids_all = []\n",
    "    attention_masks_all = []\n",
    "    token_type_ids_all = []\n",
    "    labels = []\n",
    "    slots = []\n",
    "\n",
    "\n",
    "\n",
    "    for sample in data:\n",
    "        input_ids, attention_masks, token_type_ids = model.tokenize(model.tokenizer, sample['utterance'], model.max_len)\n",
    "\n",
    "        input_ids_all.append(input_ids)\n",
    "        attention_masks_all.append(attention_masks)\n",
    "        token_type_ids_all.append(token_type_ids)\n",
    "        labels.append(sample['intent'])\n",
    "        slots.append(sample['slots'])\n",
    "\n",
    "\n",
    "    input_ids_all = input_ids_all.extend([0] * (4 - len(input_ids_all)))\n",
    "    input_ids_all = [input_ids_all]\n",
    "    input_ids_ = np.asarray(input_ids_all)\n",
    "    input_ids_ = np.expand_dims(input_ids_, axis=0)\n",
    "    input_ids_ = np.nan_to_num(input_ids_)\n",
    "    input_ids_[input_ids_ == None] = 0\n",
    "    input_ids = tf.convert_to_tensor(input_ids_, dtype=tf.int32)\n",
    "\n",
    "\n",
    "    attention_masks_all = attention_masks_all.extend([0] * (4 - len(attention_masks_all)))\n",
    "    attention_masks_all = [attention_masks_all]\n",
    "    attention_masks_ = np.asarray(attention_masks_all)\n",
    "    attention_masks_ = np.expand_dims(attention_masks_, axis=0)\n",
    "    attention_masks_ = np.nan_to_num(attention_masks_)\n",
    "    attention_masks_[attention_masks_ == None] = 0\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks_, dtype=tf.int32)\n",
    "\n",
    "    token_type_ids_all = token_type_ids_all.extend([0] * (4 - len(token_type_ids_all)))\n",
    "    token_type_ids_all = [token_type_ids_all]\n",
    "    token_type_ids_ = np.asarray(token_type_ids_all)\n",
    "    token_type_ids_ = np.expand_dims(token_type_ids_, axis=0)\n",
    "    token_type_ids_ = np.nan_to_num(token_type_ids_)\n",
    "    token_type_ids_[token_type_ids_ == None] = 0\n",
    "    token_type_ids = tf.convert_to_tensor(token_type_ids_, dtype=tf.int32)\n",
    "\n",
    "    labels = labels.extend([0] * (4 - len(labels)))\n",
    "    labels = [labels]\n",
    "    labels_ = np.asarray(labels)\n",
    "    labels_ = np.expand_dims(labels_, axis=0)\n",
    "    labels_ = np.nan_to_num(labels_)\n",
    "    labels_[labels_ == None] = 0\n",
    "    labels = tf.convert_to_tensor(labels_, dtype=tf.int32)\n",
    "\n",
    "    slots = slots.extend([0] * (4 - len(slots)))\n",
    "    slots = [slots]\n",
    "    slots_ = np.asarray(slots)\n",
    "    slots_ = np.expand_dims(slots_, axis=0)\n",
    "    slots_ = np.nan_to_num(slots_)\n",
    "    slots_[slots_ == None] = 0\n",
    "    slots = tf.convert_to_tensor(slots_, dtype=tf.int32)\n",
    "\n",
    "\n",
    "    return input_ids, attention_masks, token_type_ids, labels, slots\n",
    "\n",
    "\n",
    "train_input_ids, train_attention_masks, train_token_type_ids, train_labels, train_slots = prepare_dataset(train_raw, model)\n",
    "dev_input_ids, dev_attention_masks, dev_token_type_ids, dev_labels, dev_slots = prepare_dataset(dev_raw, model)\n",
    "test_input_ids, test_attention_masks, test_token_type_ids, test_labels, test_slots = prepare_dataset(test_raw, model)\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [train_input_ids,train_attention_masks], (train_slots, train_labels),\n",
    "    validation_data=([dev_input_ids,dev_attention_masks], (dev_slots, dev_labels)),\n",
    "    epochs=15, batch_size=128)\n",
    "\n",
    "\n",
    "result = model.evaluate([test_input_ids,test_attention_masks], (test_slots, test_labels))\n",
    "print(\"\\033[1mResults of BERT MODEL:\\033[0m\")\n",
    "print(f'Loss: {result[0]}')\n",
    "print(f'Slot Loss: {result[1]}')\n",
    "print(f'Intent Loss: {result[2]}')\n",
    "print(f'Slot Accuracy: {result[3]}')\n",
    "print(f'Intent Accuracy: {result[4]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
