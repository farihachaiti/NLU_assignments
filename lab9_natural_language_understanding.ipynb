{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in /home/disi/.local/lib/python3.8/site-packages (0.21.1)\n",
      "Requirement already satisfied: Levenshtein==0.21.1 in /home/disi/.local/lib/python3.8/site-packages (from python-Levenshtein) (0.21.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /home/disi/.local/lib/python3.8/site-packages (from Levenshtein==0.21.1->python-Levenshtein) (3.1.1)\n",
      "Requirement already satisfied: gensim in /home/disi/.local/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/disi/.local/lib/python3.8/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/disi/.local/lib/python3.8/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/disi/.local/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /home/disi/.local/lib/python3.8/site-packages (3.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib) (2.7.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/disi/.local/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /home/disi/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: tqdm in /home/disi/.local/lib/python3.8/site-packages (4.65.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement copy (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for copy\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install gensim\n",
    "!pip install matplotlib\n",
    "!pip install tqdm\n",
    "!pip install copy\n",
    "\n",
    "import os \n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path, eos_token=\"<eos>\"):\n",
    "    output = []\n",
    "    with open(path, \"r\") as f:\n",
    "        counter=0\n",
    "        for line in f.readlines():\n",
    "            output.append(line + eos_token)\n",
    "            counter += 1\n",
    "            if counter == 50: break\n",
    "    return output\n",
    "\n",
    "def get_vocab(corpus, special_tokens=[]):\n",
    "    output = {}\n",
    "    i = 0 \n",
    "    for st in special_tokens:\n",
    "        output[st] = i\n",
    "        i += 1\n",
    "    for sentence in corpus:\n",
    "        for w in sentence.split():\n",
    "            if w not in output:\n",
    "                output[w] = i\n",
    "                i += 1\n",
    "    return output\n",
    "train_raw = read_file(\"dataset/ptb.train.txt\")\n",
    "dev_raw = read_file(\"dataset/ptb.valid.txt\")\n",
    "test_raw = read_file(\"dataset/ptb.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    \n",
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "  \n",
    "    \n",
    "class LM_LSTM(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_LSTM, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        lstm_out, _  = self.lstm(emb)\n",
    "        output = self.output(lstm_out).permute(0,2,1)\n",
    "        return output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    \n",
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            output = output.reshape(output.size(0), -1)\n",
    "            target = torch.argmax(sample['target'] ,axis=1)\n",
    "            loss = eval_criterion(output, target)\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.0001 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_LSTM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 2.860798:   3%|         | 3/99 [00:02<01:16,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPPL using LSTM instead of RNN:\u001b[0m\n",
      "Test ppl:  2.7341221884655265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = model\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "                          \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print(\"\\033[1mPPL using LSTM instead of RNN:\\033[0m\")\n",
    "print('Test ppl: ', final_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "  \n",
    "    \n",
    "class LM_LSTM(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1):\n",
    "        super(LM_LSTM, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        emb2 = self.dropout1(emb)\n",
    "        lstm_out, _  = self.lstm(emb2)\n",
    "        output = self.output(lstm_out).permute(0,2,1)\n",
    "        final_output = self.dropout2(output)\n",
    "        return final_output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    \n",
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            output = output.reshape(output.size(0), -1)\n",
    "            target = torch.argmax(sample['target'] ,axis=1)\n",
    "            loss = eval_criterion(output, target)\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 200\n",
    "emb_size = 300\n",
    "\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.01 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_LSTM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"]).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]/home/disi/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:243: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  if not is_compiling() and torch.has_cuda and torch.cuda.is_available():\n",
      "PPL: 3.634206:   3%|         | 3/99 [00:00<00:03, 25.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPPL using LSTM with two dropout layers and AdamW:\u001b[0m\n",
      "Test ppl:  2.747739500527348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = model\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "for epoch in pbar:\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)    \n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "                          \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)    \n",
    "print(\"\\033[1mPPL using LSTM with two dropout layers and AdamW:\\033[0m\")\n",
    "print('Test ppl: ', final_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, log_alpha=-3.):\n",
    "        super(VariationalDropout, self).__init__()\n",
    "        self.max_log_alpha = 0.0\n",
    "        self.log_alpha = nn.Parameter(torch.Tensor([log_alpha]))\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return torch.exp(self.log_alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.train():\n",
    "            normal_noise = torch.randn_like(x)\n",
    "            self.log_alpha.data = torch.clamp(self.log_alpha.data, max=self.max_log_alpha)\n",
    "            random_tensor = 1. + normal_noise * torch.sqrt(self.alpha)\n",
    "            x *= random_tensor\n",
    "        return x\n",
    "    \n",
    "class LM_LSTM(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, output_size, pad_index=0, out_dropout=0.1,\n",
    "                 emb_dropout=0.1, n_layers=1,tie_weights=False):\n",
    "        super(LM_LSTM, self).__init__()\n",
    "        # Token ids to vectors, we will better see this in the next lab \n",
    "        self.embedding = nn.Embedding(output_size, emb_size, padding_idx=pad_index)\n",
    "        # Pytorch's RNN layer: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_size, n_layers, bidirectional=False)    \n",
    "        self.pad_token = pad_index\n",
    "        # Linear layer to project the hidden layer to our output space \n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.v_dropout = VariationalDropout()\n",
    "        if tie_weights:\n",
    "            if hidden_size != emb_size:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.embedding.weight = self.output.weight\n",
    "        \n",
    "    def forward(self, input_sequence):\n",
    "        emb = self.embedding(input_sequence)\n",
    "        lstm_out, _  = self.lstm(emb)\n",
    "        output = self.output(lstm_out).permute(0,2,1)\n",
    "        final_output = self.v_dropout(output)\n",
    "        return final_output\n",
    "    def get_word_embedding(self, token):\n",
    "        return self.embedding(token).squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "    def get_most_similar(self, vector, top_k=10):\n",
    "        embs = self.embedding.weight.detach().cpu().numpy()\n",
    "        #Our function that we used before\n",
    "        scores = []\n",
    "        for i, x in enumerate(embs):\n",
    "            if i != self.pad_token:\n",
    "                scores.append(cosine_similarity(x, vector))\n",
    "        # Take ids of the most similar tokens \n",
    "        scores = np.asarray(scores)\n",
    "        indexes = np.argsort(scores)[::-1][:top_k]  \n",
    "        top_scores = scores[indexes]\n",
    "        return (indexes, top_scores)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang():\n",
    "    def __init__(self, corpus, special_tokens=[]):\n",
    "        self.word2id = self.get_vocab(corpus, special_tokens)\n",
    "        self.id2word = {v:k for k, v in self.word2id.items()}\n",
    "        \n",
    "    def get_vocab(self, corpus, special_tokens=[]):\n",
    "        output = {}\n",
    "        i = 0 \n",
    "        for st in special_tokens:\n",
    "            output[st] = i\n",
    "            i += 1\n",
    "        for sentence in corpus:\n",
    "            for w in sentence.split():\n",
    "                if w not in output:\n",
    "                    output[w] = i\n",
    "                    i += 1\n",
    "        return output\n",
    "    \n",
    "lang = Lang(train_raw, [\"<pad>\", \"<eos>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n",
      "OOV found!\n",
      "You have to deal with that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PennTreeBank (data.Dataset):\n",
    "    # Mandatory methods are __init__, __len__ and __getitem__\n",
    "    def __init__(self, corpus, lang):\n",
    "        self.source = []\n",
    "        self.target = []\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.source.append(sentence.split()[0:-1]) # We get from the first token till the second-last token\n",
    "            self.target.append(sentence.split()[1:]) # We get from the second token till the last token\n",
    "            # See example in section 6.2\n",
    "        \n",
    "        self.source_ids = self.mapping_seq(self.source, lang)\n",
    "        self.target_ids = self.mapping_seq(self.target, lang)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src= torch.LongTensor(self.source_ids[idx])\n",
    "        trg = torch.LongTensor(self.target_ids[idx])\n",
    "        sample = {'source': src, 'target': trg}\n",
    "        return sample\n",
    "    \n",
    "    # Auxiliary methods\n",
    "    \n",
    "    def mapping_seq(self, data, lang): # Map sequences to number\n",
    "        res = []\n",
    "        for seq in data:\n",
    "            tmp_seq = []\n",
    "            for x in seq:\n",
    "                if x in lang.word2id:\n",
    "                    tmp_seq.append(lang.word2id[x])\n",
    "                else:\n",
    "                    print('OOV found!')\n",
    "                    print('You have to deal with that') # PennTreeBank doesn't have OOV but \"Trust is good, control is better!\"\n",
    "                    break\n",
    "            res.append(tmp_seq)\n",
    "        return res\n",
    "train_dataset = PennTreeBank(train_raw, lang)\n",
    "dev_dataset = PennTreeBank(dev_raw, lang)\n",
    "test_dataset = PennTreeBank(test_raw, lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "def collate_fn(data, pad_token):\n",
    "    def merge(sequences):\n",
    "        '''\n",
    "        merge from batch * sent_len to batch * max_len \n",
    "        '''\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "        max_len = 1 if max(lengths)==0 else max(lengths)\n",
    "        # Pad token is zero in our case\n",
    "        # So we create a matrix full of PAD_TOKEN (i.e. 0) with the shape \n",
    "        # batch_size X maximum length of a sequence\n",
    "        padded_seqs = torch.LongTensor(len(sequences),max_len).fill_(pad_token)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_seqs[i, :end] = seq # We copy each sequence into the matrix\n",
    "        padded_seqs = padded_seqs.detach()  # We remove these tensors from the computational graph\n",
    "        return padded_seqs, lengths\n",
    "    # Sort data by seq lengths\n",
    "\n",
    "    data.sort(key=lambda x: len(x[\"source\"]), reverse=True) \n",
    "    new_item = {}\n",
    "    for key in data[0].keys():\n",
    "        new_item[key] = [d[key] for d in data]\n",
    "\n",
    "    source, _ = merge(new_item[\"source\"])\n",
    "    target, lengths = merge(new_item[\"target\"])\n",
    "    \n",
    "    new_item[\"source\"] = source.to(device)\n",
    "    new_item[\"target\"] = target.to(device)\n",
    "    new_item[\"number_tokens\"] = sum(lengths)\n",
    "    return new_item\n",
    "\n",
    "# Dataloader instantiation\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]),  shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, collate_fn=partial(collate_fn, pad_token=lang.word2id[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_loop(data, optimizer, criterion, model, clip=5):\n",
    "    model.train()\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    \n",
    "    for sample in data:\n",
    "        optimizer.zero_grad() # Zeroing the gradient\n",
    "        output = model(sample['source'])\n",
    "        loss = criterion(output, sample['target'])\n",
    "        loss_array.append(loss.item() * sample[\"number_tokens\"])\n",
    "        number_of_tokens.append(sample[\"number_tokens\"])\n",
    "        loss.backward() # Compute the gradient, deleting the computational graph\n",
    "        # clip the gradient to avoid explosioning gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  \n",
    "        optimizer.step() # Update the weights\n",
    "        \n",
    "    return sum(loss_array)/sum(number_of_tokens)\n",
    "\n",
    "def eval_loop(data, eval_criterion, model):\n",
    "    model.eval()\n",
    "    loss_to_return = []\n",
    "    loss_array = []\n",
    "    number_of_tokens = []\n",
    "    # softmax = nn.Softmax(dim=1) # Use Softmax if you need the actual probability\n",
    "    with torch.no_grad(): # It used to avoid the creation of computational graph\n",
    "        for sample in data:\n",
    "            output = model(sample['source'])\n",
    "            \n",
    "            output = output.reshape(output.size(0), -1)\n",
    "            target = torch.argmax(sample['target'] ,axis=1)\n",
    "            loss = eval_criterion(output, target)\n",
    "            loss_array.append(loss.item())\n",
    "            number_of_tokens.append(sample[\"number_tokens\"])\n",
    "            \n",
    "    ppl = math.exp(sum(loss_array) / sum(number_of_tokens))\n",
    "    loss_to_return = sum(loss_array) / sum(number_of_tokens)\n",
    "    return ppl, loss_to_return\n",
    "\n",
    "def init_weights(mat):\n",
    "    for m in mat.modules():\n",
    "        if type(m) in [nn.GRU, nn.LSTM, nn.RNN]:\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight_ih' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.xavier_uniform_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'weight_hh' in name:\n",
    "                    for idx in range(4):\n",
    "                        mul = param.shape[0]//4\n",
    "                        torch.nn.init.orthogonal_(param[idx*mul:(idx+1)*mul])\n",
    "                elif 'bias' in name:\n",
    "                    param.data.fill_(0)\n",
    "        else:\n",
    "            if type(m) in [nn.Linear]:\n",
    "                torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
    "                if m.bias != None:\n",
    "                    m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Experiment also with a smaller or bigger model by changing hid and emb sizes \n",
    "# A large model tends to overfit\n",
    "hid_size = 200\n",
    "emb_size = 200\n",
    "\n",
    "# Don't forget to experiment with a lower training batch size\n",
    "\n",
    "# With SGD try with an higer learning rate\n",
    "lr = 0.01 # This is definitely not good for SGD\n",
    "clip = 5 # Clip the gradient\n",
    "device = 'cuda:0'\n",
    "\n",
    "vocab_len = len(lang.word2id)\n",
    "\n",
    "model = LM_LSTM(emb_size, hid_size, vocab_len, pad_index=lang.word2id[\"<pad>\"], tie_weights=True).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "\n",
    "criterion_train = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"])\n",
    "criterion_eval = nn.CrossEntropyLoss(ignore_index=lang.word2id[\"<pad>\"], reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPL: 3.316793:   3%|         | 3/99 [00:00<00:02, 43.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPPL using LSTM with variational dropout, tied weights and non-monotonic ASGD:\u001b[0m\n",
      "Test ppl:  2.73605769033373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 3\n",
    "losses_train = []\n",
    "losses_dev = []\n",
    "sampled_epochs = []\n",
    "best_ppl = math.inf\n",
    "best_model = model\n",
    "best_loss = []\n",
    "pbar = tqdm(range(1,n_epochs))\n",
    "for epoch in pbar: \n",
    "    if len(best_loss)>5:\n",
    "        optimizer = optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0., weight_decay=1.2e-6)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss = train_loop(train_loader, optimizer, criterion_train, model, clip)\n",
    "    if epoch % 1 == 0:\n",
    "        sampled_epochs.append(epoch)\n",
    "        losses_train.append(np.asarray(loss).mean())\n",
    "        ppl_dev, loss_dev = eval_loop(dev_loader, criterion_eval, model)\n",
    "        losses_dev.append(np.asarray(loss_dev).mean())\n",
    "        pbar.set_description(\"PPL: %f\" % ppl_dev)\n",
    "        if  ppl_dev < best_ppl: # the lower, the better\n",
    "            best_ppl = ppl_dev\n",
    "            best_model = copy.deepcopy(model).to('cpu')\n",
    "            patience = 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "            \n",
    "        if patience <= 0: # Early stopping with patience\n",
    "            break # Not nice but it keeps the code clean\n",
    "    best_loss.append(loss)                      \n",
    "best_model.to(device)\n",
    "final_ppl,  _ = eval_loop(test_loader, criterion_eval, best_model)   \n",
    "print(\"\\033[1mPPL using LSTM with variational dropout, tied weights and non-monotonic ASGD:\\033[0m\")\n",
    "print('Test ppl: ', final_ppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
