{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bf58e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "# A bit of preprocessing \n",
    "def preprocess(text):\n",
    "    mapping = {\"NOUN\": wordnet.NOUN, \"VERB\": wordnet.VERB, \"ADJ\": wordnet.ADJ, \"ADV\": wordnet.ADV}\n",
    "    sw_list = ['on', 'the', 'of', 'a']\n",
    "    \n",
    "    lem = WordNetLemmatizer()\n",
    "    # tokenize, if input is text\n",
    "    tokens = nltk.word_tokenize(text) if type(text) is str else text\n",
    "    # compute pos-tag\n",
    "    tagged = nltk.pos_tag(tokens, tagset='universal') # nltk.pos_tag(, tagset=\"universal\")\n",
    "    # lowercase\n",
    "    tagged = [(w.lower(), p) for w, p in tagged]\n",
    "    # optional: remove all words that are not NOUN, VERB, ADJ, or ADV (i.e. no sense in WordNet)\n",
    "    tagged = [(w, p) for w, p in tagged if p in mapping]\n",
    "    # re-map tags to WordNet (return orignal if not in-mapping, if above is not used)\n",
    "    tagged = [(w, mapping.get(p, p)) for w, p in tagged]\n",
    "    # remove stopwords\n",
    "    tagged = [(w, p) for w, p in tagged if w not in sw_list] #... not in stopword list]\n",
    "    # lemmatize\n",
    "    tagged = [(w, lem.lemmatize(w, pos=p), p) for w, p in tagged]\n",
    "    # unique the list\n",
    "    tagged = list(set(tagged))\n",
    "    \n",
    "    return tagged\n",
    "def get_sense_definitions(context):\n",
    "    # input is text or list of strings\n",
    "    lemma_tags = preprocess(context)\n",
    "\n",
    "    # let's get senses for each\n",
    "    senses = [(w, wordnet.synsets(l, p)) for w, l, p in lemma_tags]\n",
    "\n",
    "    # let's get their definitions\n",
    "    definitions = []\n",
    "    for raw_word, sense_list in senses:\n",
    "        if len(sense_list) > 0:\n",
    "            # let's tokenize, lowercase & remove stop words \n",
    "            def_list = []\n",
    "            for s in sense_list:\n",
    "                defn = s.definition()\n",
    "                # let's use the same preprocessing\n",
    "                tags = preprocess(defn)\n",
    "                toks = [l for w, l, p in tags]\n",
    "                def_list.append((s, toks))\n",
    "            definitions.append((raw_word, def_list))\n",
    "    return definitions\n",
    "    \n",
    "def get_top_sense(words, sense_list):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    val, sense = max((len(set(words).intersection(set(defn))), ss) for ss, defn in sense_list)\n",
    "    return val, sense\n",
    "\n",
    "# Lesk simplified\n",
    "def lesk_simplified(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "\n",
    "    context = set(context_sentence)\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = wordnet.synsets(ambiguous_word)\n",
    "    # Filter by pos-tag\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    _, sense = max((len(set(context).intersection(set(nltk.word_tokenize(ss.definition())))), ss) for ss in synsets)  # Don't forget to tokenize the definition\n",
    "    \n",
    "    return sense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd71411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# getting pre-computed ic of the semcor corpus (large sense tagged corpus)\n",
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('wordnet_ic')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "def get_top_sense_sim(context_sense, sense_list, similarity):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    scores = []\n",
    "    for sense in sense_list:\n",
    "        ss = sense[0]\n",
    "        if similarity == \"path\":\n",
    "            try:\n",
    "                scores.append((context_sense.path_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))    \n",
    "        elif similarity == \"lch\":\n",
    "            try:\n",
    "                scores.append((context_sense.lch_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"wup\":\n",
    "            try:\n",
    "                scores.append((context_sense.wup_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"resnik\":\n",
    "            try:\n",
    "                scores.append((context_sense.res_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"lin\":\n",
    "            try:\n",
    "                scores.append((context_sense.lin_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"jiang\":\n",
    "            try:\n",
    "                scores.append((context_sense.jcn_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        else:\n",
    "            print(\"Similarity metric not found\")\n",
    "            return None\n",
    "    val, sense = max(scores)\n",
    "    return val, sense\n",
    "\n",
    "\n",
    "def lesk_similarity(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \n",
    "                    synsets=None, majority=True):\n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # Here you may have some room for improvement\n",
    "    # For instance instead of using all the definitions from the context\n",
    "    # you pick the most common one of each word (i.e. the first)\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense_sim(sense[0], synsets, similarity))\n",
    "            \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "    \n",
    "    if majority:\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = max(scores)\n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "def original_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, majority=False):\n",
    "\n",
    "    context_senses = get_sense_definitions(set(context_sentence)-set([ambiguous_word]))\n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    scores = []\n",
    "    # print(synsets)\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense(sense[1], synsets))\n",
    "            \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "    \n",
    "    if majority:\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = max(scores)\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb807fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to\n",
      "[nltk_data]     C:\\Users\\farih\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import precision, recall, f_measure, accuracy\n",
    "nltk.download('senseval')\n",
    "from nltk.corpus import senseval\n",
    "\n",
    "# Let's create mapping from convenience\n",
    "mapping = {\n",
    "    'interest_1': 'interest.n.01',\n",
    "    'interest_2': 'interest.n.03',\n",
    "    'interest_3': 'pastime.n.01',\n",
    "    'interest_4': 'sake.n.01',\n",
    "    'interest_5': 'interest.n.05',\n",
    "    'interest_6': 'interest.n.04',\n",
    "}\n",
    "\n",
    "refs = {k: set() for k in mapping.values()}\n",
    "hyps = {k: set() for k in mapping.values()}\n",
    "hyps2 = {k: set() for k in mapping.values()}\n",
    "refs_list = []\n",
    "hyps_list = []\n",
    "hyps_list2 = []\n",
    "\n",
    "# since WordNet defines more senses, let's restrict predictions\n",
    "\n",
    "synsets = []\n",
    "for ss in wordnet.synsets('interest', pos='n'):\n",
    "    if ss.name() in mapping.values():\n",
    "        # You need to preporecess the definitions\n",
    "        # Give a look at the preprocessing function that we defined above \n",
    "        defn = ss.definition()\n",
    "        # let's use the same preprocessing\n",
    "        tags = preprocess(defn)\n",
    "        toks = [l for w, l, p in tags]\n",
    "        synsets.append((ss,toks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b086767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w-2_word': 'declines', 'w-1_word': 'in', 'w+1_word': 'rates', 'w+2_word': '.', 'pos-tag for w-2_word': 'NOUN', 'pos-tag for w-1_word': 'NOUN', 'pos-tag for w+1_word': 'VERB', 'pos-tag for w+2_word': '.', 'ngrams within w-2': ('declines', 'NNS'), 'ngrams within w-1': ('in', 'IN'), 'ngrams within w+1': ('rates', 'NNS'), 'ngrams within w+2': ('.', '.')}\n",
      "\u001b[1mEvaluation score for Concatenated BAO and Extended Collocational Feature Vectors:\u001b[0m\n",
      "0.906\n",
      "\u001b[1mFor Original Lesk:\u001b[0m Acc: 0.046\n",
      "\u001b[1mFor Lesk Similarity:\u001b[0m Acc: 0.065\n",
      "\u001b[1mEvaluation score For Original Lesk:\u001b[0minterest.n.04  : p=0.989; r=0.070; f=0.989; s=1252\n",
      "\u001b[1mEvaluation score For Lesk Similarity:\u001b[0minterest.n.04  : p=1.000; r=0.005; f=1.000; s=1252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.util import ngrams\n",
    "import numpy as np\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "lblencoder = LabelEncoder()\n",
    "data = [\" \".join([t[0] for t in inst.context]) for inst in senseval.instances('interest.pos')]\n",
    "lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]\n",
    "stratified_split = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "lblencoder.fit(lbls)\n",
    "labels = lblencoder.transform(lbls)\n",
    "\n",
    "def collocational_features(inst):\n",
    "    p = inst.position\n",
    "    return {\n",
    "        \"w-2_word\": 'NULL' if p < 2 else inst.context[p-2][0],\n",
    "        \"w-1_word\": 'NULL' if p < 1 else inst.context[p-1][0],\n",
    "        \"w+1_word\": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][0],\n",
    "        \"w+2_word\": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][0],\n",
    "        \"pos-tag for w-2_word\": 'NULL' if p < 2 else nltk.pos_tag(inst.context[p-2], tagset=\"universal\")[1][1],\n",
    "        \"pos-tag for w-1_word\": 'NULL' if p < 1 else nltk.pos_tag(inst.context[p-1], tagset=\"universal\")[1][1],\n",
    "        \"pos-tag for w+1_word\": 'NULL' if len(inst.context) - 1 < p+1 else nltk.pos_tag(inst.context[p+1], tagset=\"universal\")[1][1],\n",
    "        \"pos-tag for w+2_word\": 'NULL' if len(inst.context) - 1 < p+2 else nltk.pos_tag(inst.context[p+2], tagset=\"universal\")[1][1],\n",
    "        \"ngrams within w-2\": 'NULL' if p < 2 else list(ngrams(inst.context[p-2], 2))[0],\n",
    "        \"ngrams within w-1\": 'NULL' if p < 1 else list(ngrams(inst.context[p-1], 2))[0],\n",
    "        \"ngrams within w+1\": 'NULL' if len(inst.context) - 1 < p+1 else list(ngrams(inst.context[p+1], 2))[0],\n",
    "        \"ngrams within w+2\": 'NULL' if len(inst.context) - 1 < p+2 else list(ngrams(inst.context[p+2], 2))[0]\n",
    "    }\n",
    "\n",
    "data_col = [collocational_features(inst) for inst in senseval.instances('interest.pos')]\n",
    "print(data_col[0])\n",
    "\n",
    "\n",
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "dvectors = dvectorizer.fit_transform(data_col)\n",
    "\n",
    "uvectors = np.concatenate((vectors.toarray(), dvectors), axis=1)\n",
    "scores = cross_validate(classifier, uvectors, labels, cv=stratified_split, scoring=['f1_micro'])\n",
    "print(\"\\033[1mEvaluation score for Concatenated BAO and Extended Collocational Feature Vectors:\\033[0m\")\n",
    "print(\"{:.3f}\".format(sum(scores['test_f1_micro'])/len(scores['test_f1_micro'])))\n",
    "\n",
    "\n",
    "for i, inst in enumerate(senseval.instances('interest.pos')):\n",
    "    txt = [t[0] for t in inst.context]\n",
    "    raw_ref = inst.senses[0] # let's get first sense\n",
    "    hyp = original_lesk(txt, txt[inst.position], synsets=synsets, majority=True).name()\n",
    "    hyp2 = lesk_similarity(txt, txt[inst.position], similarity=\"resnik\", synsets=synsets, majority=True).name()\n",
    "    ref = mapping.get(raw_ref)\n",
    "    \n",
    "    # for precision, recall, f-measure        \n",
    "    refs[ref].add(i)\n",
    "    hyps[hyp].add(i)\n",
    "    hyps2[hyp2].add(i)\n",
    "    \n",
    "    \n",
    "    # for accuracy\n",
    "    refs_list.append(ref)\n",
    "    hyps_list.append(hyp)\n",
    "    hyps_list2.append(hyp2)\n",
    "\n",
    "print(\"\\033[1mFor Original Lesk:\\033[0m Acc:\", round(accuracy(refs_list, hyps_list), 3))\n",
    "print(\"\\033[1mFor Lesk Similarity:\\033[0m Acc:\", round(accuracy(refs_list, hyps_list2), 3))\n",
    "\n",
    "for cls in hyps.keys():\n",
    "    p = precision(refs[cls], hyps[cls])\n",
    "    r = recall(refs[cls], hyps[cls])\n",
    "    f = f_measure(refs[cls], hyps[cls], alpha=1)\n",
    "print(\"\\033[1mEvaluation score For Original Lesk:\\033[0m{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}\".format(cls, p, r, f, len(refs[cls])))\n",
    "\n",
    "for cls in hyps2.keys():  \n",
    "    p = precision(refs[cls], hyps2[cls])\n",
    "    r = recall(refs[cls], hyps2[cls])\n",
    "    f = f_measure(refs[cls], hyps2[cls], alpha=1)\n",
    "print(\"\\033[1mEvaluation score For Lesk Similarity:\\033[0m{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}\".format(cls, p, r, f, len(refs[cls])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620dedd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
